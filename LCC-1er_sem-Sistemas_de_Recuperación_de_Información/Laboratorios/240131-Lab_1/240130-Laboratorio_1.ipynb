{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBpOGbjmEjBV"
      },
      "source": [
        "# Laboratorio \\#1: Pre-procesamiento léxico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5bkXzvZEptk"
      },
      "source": [
        "El Procesamiento del Lenguaje Natural (NLP) es una rama de la inteligencia artificial que se centra en la interacción entre las computadoras y el lenguaje humano, buscando la manera de leer, comprender y hacer que las computadoras interpreten el lenguaje humano de manera útil. El preprocesamiento léxico es un paso crítico en NLP, involucrando la preparación y limpieza inicial del texto. Esta fase incluye tareas como la tokenización, la eliminación de ruido, la eliminación de stop-words, la reducción morfológica, el filtrado por ocurrencia y la construcción de vocabulario. Estos pasos son esenciales para transformar un texto \"crudo\" en un objeto con formato estructurado y analizable, eliminando la información irrelevante y destacando los elementos significativos del lenguaje."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW0__Z1AEtqk"
      },
      "source": [
        "# Configurando el entorno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCz9NcBeEttG"
      },
      "source": [
        "### spacy\n",
        "\n",
        "spaCy es una biblioteca avanzada y de alto rendimiento en el campo del Procesamiento del Lenguaje Natural (NLP), diseñada específicamente para aplicaciones de producción. Se destaca por su eficiencia y velocidad, ofreciendo una serie de modelos pre-entrenados optimizados para diversas tareas de NLP como la tokenización, el etiquetado de las partes del discurso, el reconocimiento de entidades nombradas y el análisis de dependencias. Una de las fortalezas clave de spaCy es su capacidad para manejar grandes volúmenes de texto de manera eficiente, lo que la hace ideal para proyectos que requieren un procesamiento de lenguaje natural a gran escala. Además, spaCy integra características como el procesamiento por lotes y la capacidad de personalización de modelos, lo que facilita su incorporación en flujos de trabajo complejos y sistemas de producción en tiempo real. Su enfoque pragmático y orientado a la industria hace de spaCy una herramienta esencial para desarrolladores y científicos de datos que buscan precisión y rendimiento en sus aplicaciones de NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUgqYTT5EtwP"
      },
      "source": [
        "### nltk\n",
        "\n",
        "NLTK, por otro lado, es una biblioteca de NLP ampliamente reconocida por su enfoque educativo y de investigación. Es una herramienta de gran valor para aprender y enseñar procesamiento del lenguaje natural, gracias a su extensa colección de recursos lingüísticos y herramientas de análisis. NLTK proporciona acceso a una diversidad de corpus textuales y recursos de conocimiento como WordNet, facilitando el estudio de diferentes aspectos del lenguaje y sus aplicaciones computacionales. Además, incluye una amplia gama de herramientas para tareas fundamentales de NLP, como la tokenización, el etiquetado de partes del discurso, y el análisis sintáctico. La biblioteca se distingue por su enfoque didáctico, con amplia documentación y tutoriales que la hacen accesible para principiantes en NLP, así como una opción robusta para investigadores que buscan una plataforma para experimentar con técnicas de procesamiento del lenguaje. A pesar de no estar específicamente orientada a aplicaciones de producción, NLTK sigue siendo una herramienta fundamental en el campo del NLP, especialmente en entornos académicos y de investigación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M13H0YWE3FK"
      },
      "source": [
        "### gensim\n",
        "\n",
        "Gensim es una biblioteca poderosa y versátil para modelado de temas, recuperación de información, y tareas de análisis semántico en el campo del NLP. Una de sus características más destacadas es el acceso a corpus y la capacidad de procesamiento de estos, permitiendo a los usuarios trabajar con grandes colecciones de documentos de texto de manera eficiente. Gensim también sobresale en la implementación de modelos de tópicos como Latent Dirichlet Allocation (LDA) y Latent Semantic Indexing (LSI), que son fundamentales para descubrir la estructura temática subyacente en grandes colecciones de texto. Además, Gensim proporciona funcionalidades para trabajar con embeddings de palabras, como Word2Vec, FastText y Doc2Vec, herramientas esenciales para capturar el significado semántico de las palabras y documentos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cYDmCZ3E4fa"
      },
      "outputs": [],
      "source": [
        "# Instalando los recursos básicos, en caso de no contar con ellos\n",
        "\n",
        "try:\n",
        "    import ir_datasets\n",
        "except:\n",
        "    !pip install ir-datasets\n",
        "\n",
        "try:\n",
        "    import nltk\n",
        "except:\n",
        "    !pip install nltk\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "try:\n",
        "    import spacy\n",
        "    spacy.load(\"en_core_web_sm\")\n",
        "except:\n",
        "    !pip install spacy\n",
        "    !python -m spacy download en_core_web_sm\n",
        "\n",
        "try:\n",
        "    import gensim\n",
        "except:\n",
        "    !pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6H7avFxwE-8L"
      },
      "outputs": [],
      "source": [
        "# Cargando las bibliotecas\n",
        "import ir_datasets\n",
        "import nltk\n",
        "import spacy\n",
        "import gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgL4hAa4FINR"
      },
      "source": [
        "# Cargando el corpus\n",
        "\n",
        "### Cranfield\n",
        "\n",
        "El corpus Cranfield es un conjunto de datos clásico en el campo de la Recuperación de Información, compuesto por aproximadamente 1,400 resúmenes de artículos de investigación en aerodinámica. Cada documento en este corpus incluye un título, un resumen conciso del contenido, y en algunos casos, palabras clave y referencias bibliográficas. Acompañando a estos documentos, hay alrededor de 225 consultas de prueba y sus correspondientes juicios de relevancia, proporcionados por expertos, que indican la pertinencia de cada documento para una consulta específica. Este diseño estructurado y su enfoque específico en temas de aerodinámica hacen del corpus una herramienta esencial para evaluar la eficacia de Sistemas de Recuperación de Información, sirviendo como un modelo estándar para pruebas comparativas y consistentes en esta disciplina."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a80NAtSGFCYG"
      },
      "outputs": [],
      "source": [
        "# Cargar el corpus y mostrar un documento\n",
        "dataset = ir_datasets.load(\"cranfield\")\n",
        "\n",
        "documents = [doc.text for doc in dataset.docs_iter()]\n",
        "\n",
        "print(documents[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2gs4mSOF3Cc"
      },
      "source": [
        "Nota: Para los efectos de esta clase práctica solo es necesario tomar el título y contenido de los documentos, y se obvian los metadatos como la fecha, el autor, etc., que podrían ser importantes en otros análisis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ULvnnRiFb4o"
      },
      "source": [
        "## 1. Tokenización\n",
        "\n",
        "La **tokenización** es el proceso de convertir un texto en una secuencia de *tokens*, que son piezas más pequeñas, como palabras, números o símbolos. Constituye la base para el análisis del texto y el NLP, al transformar el lenguaje natural, complejo y variado, en una forma estructurada y analizable. Es el primer paso para descomponer el texto en unidades manejables, permitiendo procesos más complejos como el análisis sintáctico y semántico. Al dividir el texto en tokens, se simplifica la tarea de identificar patrones, entidades nombradas, temas o sentimientos. Además, permite convertir el texto en una forma tal que los modelos pueden \"entender\", como vectores de características."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7chPgTG5FStU"
      },
      "outputs": [],
      "source": [
        "tokenized_docs = []\n",
        "vector_repr = []\n",
        "dictionary = {}\n",
        "vocabulary = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5CAMHiWF_uL"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def tokenization_spacy(texts):\n",
        "  raise NotImplementedError()\n",
        "\n",
        "tokenization_spacy(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dlzNRrmIaxP"
      },
      "outputs": [],
      "source": [
        "def tokenization_nltk(texts):\n",
        "    global tokenized_docs\n",
        "    raise NotImplementedError()\n",
        "\n",
        "tokenization_nltk(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue4hEnxbLHIt"
      },
      "source": [
        "## 2. Eliminación de Ruido\n",
        "\n",
        "La **eliminación de ruido** implica descartar elementos no esenciales como signos de puntuación, números, y caracteres especiales. Este proceso ayuda a enfocar el análisis en el contenido relevante. Al eliminar estos elementos, se reduce la complejidad (dimensión) del texto y se facilita la identificación de palabras clave y patrones significativos. Además, la eliminación de ruido es especialmente importante en tareas como la clasificación de texto, la extracción de información y el análisis de sentimientos, donde la presencia de elementos irrelevantes pueden distorsionar los resultados, reducir la precisión de los modelos de NLP y conllevar a resultados no deseados por la mala selección de las características."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03uebzkwKkUQ"
      },
      "outputs": [],
      "source": [
        "def remove_noise_spacy(tokenized_docs):\n",
        "  raise NotImplementedError()\n",
        "\n",
        "remove_noise_spacy(tokenization_spacy(documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxks-T9zLzDI"
      },
      "outputs": [],
      "source": [
        "def remove_noise_nltk(tokenized_docs):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "remove_noise_nltk(tokenized_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm509LZyUcHH"
      },
      "source": [
        "## 3. Eliminación de Stop-Words:\n",
        "\n",
        "La **eliminación de _stop-words_** es un proceso clave en el análisis de texto y el NLP. Los _stop-words_ son palabras comunes que, aunque necesarias para la estructura gramatical, suelen carecer de significado específico, como \"el\", \"la\", \"de\", \"y\", \"en\". Eliminar estas palabras del texto ayuda a centrarse en las palabras que realmente contribuyen al significado y contexto. Este paso es fundamental para mejorar la eficiencia de los algoritmos de búsqueda y procesamiento, ya que reduce el tamaño del texto a analizar y resalta términos más relevantes y específicos. Además, en tareas como la extracción de características, clasificación de texto, y análisis de sentimientos, la eliminación de _stop-words_ puede aumentar significativamente la precisión y relevancia de los resultados.\n",
        "\n",
        "En ciertas ocaciones los tokens definidos en los _stop-words_ son insuficientes y es necesaria ampliar dicha lista con otros tokens propios al corpus que se desea \"limpiar\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMg4MANbL1wJ"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords_spacy(tokenized_docs):\n",
        "  raise NotImplementedError()\n",
        "\n",
        "remove_stopwords_spacy(remove_noise_spacy(tokenization_spacy(documents)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWs6NQsvUvLL"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(tokenized_docs):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "remove_stopwords(tokenized_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELA3YzszWLk-"
      },
      "source": [
        "## 4. Reducción Morfológica\n",
        "\n",
        "La **reducción morfológica** incluye técnicas como la lematización y el stemming. La **lematización** implica reducir las palabras a su forma base o lema, considerando el análisis morfológico de las palabras. Esto significa que palabras como \"corriendo\" y \"corrió\" se reducirían a su lema común \"correr\", lo que ayuda a mantener el significado contextual en el análisis. Por otro lado, el **stemming** reduce las palabras a su raíz o tallo, a menudo mediante la eliminación de sufijos y prefijos, lo que puede ser menos preciso que la lematización pero es más rápido y simple. Por ejemplo, \"corriendo\" y \"corrió\" podrían reducirse a \"corr\". Ambas técnicas son importantes para normalizar las variantes de las palabras (vocabulario) y facilitar la identificación de patrones en el texto. Mientras que la lematización es más adecuada para tareas que requieren precisión lingüística, el stemming es útil para aplicaciones que necesitan rapidez y eficiencia en el procesamiento de grandes volúmenes de datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oktnHo2UMtKm"
      },
      "outputs": [],
      "source": [
        "def morphological_reduction_spacy(tokenized_docs, use_lemmatization=True):\n",
        "  raise NotImplementedError()\n",
        "\n",
        "morphological_reduction_spacy(remove_stopwords_spacy(remove_noise_spacy(tokenization_spacy(documents))), True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTMNWVJIRHfB"
      },
      "outputs": [],
      "source": [
        "def morphological_reduction_nltk(tokenized_docs, use_lemmatization=True):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "morphological_reduction_nltk(tokenized_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-JJYnTJXr6Y"
      },
      "source": [
        "## 5. Filtrado según Ocurrencia:\n",
        "\n",
        "El **filtrado según ocurrencia** implica seleccionar o descartar palabras basándose en la frecuencia con la que aparecen en el texto. Permite identificar y enfocarse en términos que son significativos dentro de un corpus específico. Palabras que aparecen con mucha frecuencia (términos comunes) o con muy poca frecuencia (palabras raras) a menudo se eliminan. La razón es que las palabras muy comunes suelen aportar poco valor analítico y las muy raras pueden ser menos relevantes o incluso errores. El filtrado por ocurrencia ayuda a mejorar la relevancia y la calidad de los análisis de texto, simplificando los datos y resaltando las palabras que son verdaderamente representativas del tema o contenido analizado. En aplicaciones prácticas, como en la clasificación de texto o en el modelado de temas, este enfoque facilita la extracción de insights significativos y mejora la eficacia de los algoritmos de NLP al reducir el ruido y la dimensionalidad de los datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITVapHetWcoW"
      },
      "outputs": [],
      "source": [
        "def filter_tokens_by_occurrence(tokenized_docs, no_below=5, no_above=0.5):\n",
        "  global dictionary\n",
        "  raise NotImplementedError()\n",
        "\n",
        "tokenized_docs = filter_tokens_by_occurrence(tokenized_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J4pcuw0Zh70"
      },
      "source": [
        "## 6. Construcción del Vocabulario\n",
        "\n",
        "La **construcción del vocabulario** es un paso fundamental en el procesamiento del lenguaje natural (NLP) y en el análisis de textos. Este proceso implica crear un conjunto de palabras únicas, o 'vocabulario', a partir de un corpus. La construcción de un vocabulario eficaz es crucial porque sirve como base para muchas tareas de NLP, como la vectorización de texto, el modelado de temas, y la clasificación de texto. Al definir un vocabulario, se establece un marco uniforme que permite convertir el texto en representaciones numéricas, facilitando así el procesamiento por parte de algoritmos de aprendizaje automático. Este conjunto de palabras únicas también ayuda a identificar y analizar las características más importantes de un corpus, lo que resulta vital para comprender y extraer insights significativos del texto. La construcción de un vocabulario adecuado requiere un equilibrio entre incluir palabras suficientemente representativas y excluir aquellas que no aportan información relevante, asegurando así la eficiencia y precisión en el análisis posterior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYtOFVnJY31i"
      },
      "outputs": [],
      "source": [
        "def build_vocabulary(dictionary):\n",
        "  raise NotImplementedError()\n",
        "\n",
        "vocabulary = build_vocabulary(dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtN2JGXYdb0E"
      },
      "source": [
        "## 7. Representación Vectorial de Documentos\n",
        "\n",
        "En el procesamiento del lenguaje natural (NLP), la **representación vectorial de documentos** es un proceso clave que transforma texto en vectores numéricos, facilitando su análisis por algoritmos de aprendizaje automático. Existen varios métodos para esta representación. El más simple es el modelo Bag of Words (BoW), donde cada documento se representa como un vector que indica la presencia o frecuencia de palabras del vocabulario en el documento. Una variante más avanzada es el modelo TF-IDF (Frecuencia de Término - Frecuencia Inversa de Documento), que refleja no solo la frecuencia de las palabras, sino también su importancia en el documento y el corpus en general. Métodos más sofisticados incluyen modelos de word embeddings, como Word2Vec y GloVe, que capturan relaciones semánticas y contextuales entre palabras, representándolas como vectores densos en un espacio de características de alta dimensión. También existen BERT Embeddings, derivados del modelo de lenguaje BERT, que proporcionan representaciones contextuales de palabras basadas en su uso en oraciones específicas. Cada uno de estos métodos tiene sus fortalezas y aplicaciones, y la elección de uno sobre otro depende de la tarea específica de NLP y los requisitos de precisión y complejidad del análisis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSudNTfCaIno"
      },
      "outputs": [],
      "source": [
        "def vector_representation(tokenized_docs, dictionary, vector_repr, use_bow=True):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "vector_repr = vector_representation(tokenized_docs, dictionary, vector_repr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JTthQ0od4Gu"
      },
      "source": [
        "## 8. Etiquetado de las parte del discurso\n",
        "\n",
        "El etiquetado de las parte del discurso (POS tagging) implica asignar categorías gramaticales (como sustantivo, verbo, adjetivo) a cada palabra en un texto. Este proceso permite determinar el significado y la función de las palabras en diferentes contextos, lo que es fundamental para tareas como la desambiguación de significados, el análisis sintáctico, la extracción de información y la traducción automática. Al identificar correctamente las partes del discurso, el etiquetado POS mejora la precisión y eficacia en la comprensión y el análisis del lenguaje humano por parte de los sistemas informáticos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBG685K9dy2n"
      },
      "outputs": [],
      "source": [
        "def pos_tagger_spacy(tokenized_docs):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "pos_tags = pos_tagger_spacy(tokenization_spacy(documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ug5yLWFBe2ff"
      },
      "outputs": [],
      "source": [
        "def pos_tagger_nltk(tokenized_docs):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "pos_tags = pos_tagger_nltk(tokenized_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWxDEJTfaeb8"
      },
      "source": [
        "Prueba visual de los tokens características para cierto documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vioWJLIwZMzv"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from wordcloud import WordCloud\n",
        "except:\n",
        "  !pip install wordcloud\n",
        "\n",
        "try:\n",
        "  import matplotlib.pyplot as plt\n",
        "except:\n",
        "  !pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unhShT3lZUNR"
      },
      "outputs": [],
      "source": [
        "def show(document):\n",
        "  text = ' '.join(document)\n",
        "  wordcloud = WordCloud(width = 800, height = 800,\n",
        "                background_color ='white',\n",
        "                min_font_size = 10).generate(text)\n",
        "\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "\n",
        "show(tokenized_docs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSmCqH3edC7F"
      },
      "source": [
        "Si considera que los tokens para el documento graficado en la **nube de palabras** no son lo suficientemente descriptivos, pruebe añadir un filtro pero esta vez sobre los tipos de tokens: sustantivos, verbos, artículos, conjunciones, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE3af2Fff7WL"
      },
      "source": [
        "# Conclusiones\n",
        "\n",
        "Si se busca velocidad y eficiencia para aplicaciones en producción, spaCy es probablemente la mejor opción. Sin embargo, si el enfoque está en la educación, la investigación o se necesita una gama más amplia de recursos lingüísticos, NLTK podría ser más adecuado. En muchos casos, se utilizan ambas bibliotecas en diferentes etapas de sus proyectos para aprovechar las fortalezas de cada una.\n",
        "\n",
        "En particular para el pre-procesamiento léxico spaCy suele ser la elección preferida por su rendimiento y precisión."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
