{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4prRyuQ4e6O"
      },
      "outputs": [],
      "source": [
        "!pip install ir-datasets scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zKShlyI-pLK"
      },
      "source": [
        "# Laboratorio #3: Medidas de evaluación\n",
        "\n",
        "Las medidas de evaluación desempeñan un papel fundamental en los Sistemas de Recuperación de Información (SRI) al proporcionar una forma objetiva de medir su rendimiento y efectividad. Estas medidas permiten a las personas comprender y mejorar la calidad de los resultados recuperados por el sistema.\n",
        "\n",
        "Como importancia del uso de las medidas de evaluación, se pueden mencionar:\n",
        "\n",
        "1. **Calidad de los Resultados**: Las medidas de evaluación permiten determinar la precisión y relevancia de los documentos recuperados en relación con las consultas de los usuarios. Cuanto más precisos y relevantes sean los resultados, mayor será la utilidad y satisfacción del usuario.\n",
        "\n",
        "2. **Optimización del Sistema**: Al proporcionar métricas cuantitativas, las medidas de evaluación permiten a los desarrolladores identificar áreas de mejora en el SRI. Esto puede implicar ajustes en algoritmos de búsqueda, ponderación de términos, o mejoras en la indexación de documentos, entre otros.\n",
        "\n",
        "3. **Comparación de Métodos y Sistemas**: Las medidas de evaluación facilitan la comparación entre diferentes métodos de búsqueda y sistemas de recuperación de información. Esto es fundamental para seleccionar la mejor solución en función de los requisitos específicos del usuario y del contexto de aplicación.\n",
        "\n",
        "No obstante, crear los conjuntos de datos para aplicar las distintas métricas y evaluar el sistema es un laborioso trabajo que solo puede ser llevado por expertos en el tema.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpmya3ShM0K1"
      },
      "source": [
        "Para ser consecuentes, continuaremos trabajando con los documentos del corpus 'Cranfield'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGHAdL-VMxP9"
      },
      "outputs": [],
      "source": [
        "import ir_datasets\n",
        "# dataset = ir_datasets.load(\"cranfield\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RYlQpX9NHfw"
      },
      "source": [
        "La variable **dataset**, instancia de la clase **ir_datasets.datasets.base.Dataset**, tiene 3 funciones con las cuales se estarán trabajando en esta clase. Estas son:\n",
        "\n",
        "1. **docs_iter()**: Devuelve un objeto iterable de tuplas de dimensión 5, referentes a los documentos. Los campos de cada tupla son:\n",
        "  - Identificador (str)\n",
        "  - Título (str)\n",
        "  - Texto (str)\n",
        "  - Autor (str)\n",
        "  - Referencia bibliográfica (str)\n",
        "  \n",
        "2. **queries_iter()**: Devuelve un objeto iterable de tuplas de dimensión 2, referentes a las consultas consultas predefinidas. Los campos de cada tupla son:\n",
        "  - Identificador (str)\n",
        "  - Text (str)\n",
        "  \n",
        "3. **qrels_iter()**: Devuelve un objeto iterable de tuplas de dimensión 5, referentes al nivel de relevancia de los documentos dada una consulta. Los campos de cada tupla son:\n",
        "  - Identificador de la consulta (str)\n",
        "  - Identificador del documento (str)\n",
        "  - Relevancia (int)\n",
        "    - -1: Sin importancia o valor\n",
        "    - 1: Referencia de interés mínimo\n",
        "    - 2: Referencia útil\n",
        "    - 3: Referencia de alto grado de relevancia\n",
        "    - 4: Referencia que responde a la consulta\n",
        "  - Número de iteración (int = 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usando sklearn \n",
        "\n",
        "Scikit-learn, comúnmente conocido como `sklearn`, es una biblioteca de software de código abierto que proporciona herramientas simples y eficientes para el análisis de datos y la modelización estadística. Esta biblioteca es ampliamente utilizada en el campo del aprendizaje automático para la implementación de algoritmos de clasificación, regresión, agrupamiento, y reducción de dimensionalidad. Además, `sklearn` puede utilizarse para medir el desempeño de un SRI a través de métricas de evaluación como precisión, recall, y F1-score, o la matriz de confusión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "help(confusion_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ejemplo: Asumamos que el corpus cuente con solo 10 documentos y para cierta consulta se tiene los documentos recuperados, siendo estos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Etiquetas verdaderas\n",
        "#   0 -> documento relevante\n",
        "#   1 -> documento irrelevante\n",
        "y_true = [0, 1, 1, 0, 1, 0, 1, 0, 0, 1]  \n",
        "\n",
        "# Predicciones del modelo SRI\n",
        "#   0 -> documento recuperado\n",
        "#   1 -> documento no recuperado\n",
        "y_pred = [0, 1, 1, 0, 0, 0, 1, 1, 0, 0]  # Predicciones del modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matriz de Confusión\n",
        "\n",
        "La matriz de confusión es una herramienta de análisis que permite la evaluación del desempeño de un sistema de clasificación. Se estructura en cuatro componentes esenciales:\n",
        "\n",
        "* Verdaderos Positivos (TP): Número de elementos correctamente identificados como relevantes.\n",
        "* Falsos Positivos (FP): Número de elementos incorrectamente identificados como relevantes.\n",
        "* Verdaderos Negativos (TN): Número de elementos correctamente identificados como no relevantes.\n",
        "* Falsos Negativos (FN): Número de elementos incorrectamente identificados como no relevantes.\n",
        "\n",
        "Esta matriz proporciona una base para calcular otras métricas de evaluación de desempeño."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matrix = confusion_matrix(y_true, y_pred)\n",
        "tn, fp, fn, tp = matrix.ravel()\n",
        "\n",
        "print(matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Precisión\n",
        "\n",
        "La precisión indica qué proporción de los elementos recuperados son relevantes para la consulta realizada y se calcula como:\n",
        "$$ P = \\frac{|TP|}{|TP| + |FP|} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "precision = precision_score(y_true, y_pred)\n",
        "print(f\"Precisión: {precision}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Nota:** Una precisión de 1 no necesariamente implica que el sistema haya recuperado todos los elementos relevantes disponibles. La precisión se enfoca exclusivamente en la calidad de los resultados recuperados, no en la completitud. Por lo tanto, un sistema podría tener una precisión perfecta pero un recobrado bajo si solo recupera una pequeña fracción de todos los ítems relevantes disponibles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recobrado\n",
        "\n",
        "El recobrado representa la proporción de elementos relevantes que fueron recuperados con respecto a todos los elementos relevantes existentes en el conjunto de datos, a partir de la consulta. Se calcula como:\n",
        "$$ \\text{R} = \\frac{|TP|}{|TP| + |FN|} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "recall = recall_score(y_true, y_pred)\n",
        "print(f\"Recobrado: {recall}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Nota:** Un recobrado de 1 no indica nada sobre la cantidad de elementos no relevantes que también puedan haber sido recuperados (esto sería evaluado por la precisión). Por lo tanto, es posible tener un recobrado de 1 con una precisión baja si el sistema recupera muchos elementos irrelevantes junto con todos los relevantes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Medida F\n",
        "\n",
        "La medida F es una métrica que combina precisión y recobrado en un solo valor, lo que proporciona una evaluación equilibrada del sistema. Se calcula como la media armónica de estas medidas:\n",
        "\n",
        "$$ F = \\frac{(1 + \\beta^2) \\cdot \\text{P} \\cdot \\text{R}}{\\beta^2 \\cdot \\text{P} + \\text{R}} $$\n",
        "\n",
        "donde **β** es un factor el cual indica la medida a prestarle más atención o importancia.\n",
        "\n",
        "Cuando:\n",
        "- **β > 1**, el recobrado se considera más importante que la precisión. Por ejemplo, F2 pone más énfasis en el recobrado.\n",
        "- **β < 1**, la precisión se considera más importante. Por ejemplo, F0.5 pone más énfasis en la precisión.\n",
        "- **β = 1**, la precisión y el recobrado se consideran igualmente importantes, lo que nos lleva a la medida F1.\n",
        "\n",
        "### Medida F1\n",
        "\n",
        "$$ F1 = \\frac{2 \\cdot \\text{P} \\cdot \\text{R}}{\\text{P} + \\text{R}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f1 = f1_score(y_true, y_pred)\n",
        "print(f\"Medida F1: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Nota:** La razón por la que se utiliza la media armónica en lugar de la media aritmética es que la media armónica penaliza más fuertemente los valores extremos. Esto significa que si uno de los dos valores (precisión o recobrado) es muy bajo, la medida F1 también será baja, lo que refleja una necesidad de equilibrio entre capturar todos los elementos relevantes (recobrado alto) y asegurar que los elementos recuperados sean relevantes (alta precisión)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## R-Precisión\n",
        "\n",
        "La **R-Precisión** es una métrica específica utilizada para evaluar la efectividad de SRI, enfocándose en los primeros R resultados devueltos por una consulta. Esta medida evalúa la proporción de elementos relevantes encontrados dentro de los primeros R elementos recuperados, donde R es un número predefinido que representa un umbral de interés para el evaluador o el contexto de uso. \n",
        "\n",
        "La R-Precisión es particularmente relevante en aplicaciones donde los usuarios tienden a considerar solo los primeros resultados de una búsqueda, como en motores de búsqueda en Internet o en sistemas de recomendación. Al medir la precisión en este subconjunto inicial de resultados, la medida proporciona una visión clara de la calidad y relevancia de los elementos que los usuarios ven primero, lo que es crucial para la satisfacción del usuario y la eficacia general del sistema en entornos prácticos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# El primer paso es ordenar de forma descendente según probabilidad las listas y_true y y_pred \n",
        "r = 8\n",
        "r_precision = precision_score(y_true, y_pred[:r] + [0] * (len(y_pred) - r))\n",
        "print(f\"R-Precisión: {r_precision}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Proporción de fallo\n",
        "\n",
        "La proporción de fallo es una métrica que cuantifica la tasa de falsos positivos, es decir, evalúa qué porcentaje de los elementos no relevantes ha sido erróneamente clasificado como relevante por el sistema, en relación con el total de elementos no relevantes. Se expresa mediante la fórmula:\n",
        "$$ \\text{Fallout} = \\frac{|FP|}{|FP| + |TN|} $$\n",
        "Esta medida es importante para evaluar la proporción de ruido generado por el sistema al recuperar información no pertinente.\n",
        "\n",
        "Esta medida no se encuentra en sklearn puesto que es propia de los SRI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fallout = fp / (fp + tn)\n",
        "print(f\"Fallout: {fallout}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Luego, sklearn provee la mayoría de las medidas necesarias pero tiene un problema: necesita todo el conjunto de datos clasificado. Esto es un problema en cuanto al espacio de memoria cuando conjunto de datos excede la cifra de los millones, el cual es un número común en sistemas que trabajan, cargan y procesan datos constantemente de Internet.\n",
        "\n",
        "Por tanto, **se requiere que implemente cada métrica pero optimizadas a los subconjuntos de datos relevantes y datos recuperados**. \n",
        "\n",
        "Para ello, se brinda 2 funciones que puede utilizar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relevant_documents(query_id : str):\n",
        "  \"\"\"\n",
        "  Returns relevant documents given a query and the query\n",
        "  \n",
        "  Args: \n",
        "    - query_id (str) : Query identifier.\n",
        "\n",
        "  Return: \n",
        "    list<str>\n",
        "  \n",
        "  \"\"\"\n",
        "  for (queryt_id, query_text) in dataset.queries_iter():\n",
        "    if queryt_id == query_id:\n",
        "      break\n",
        "    \n",
        "  return (\n",
        "    [\n",
        "      doc_id\n",
        "      for (queryt_id, doc_id, relevance, iteration) in dataset.qrels_iter()\n",
        "      if queryt_id == query_id and relevance in [3, 4]\n",
        "    ], \n",
        "    query_text)\n",
        "\n",
        "import random\n",
        "def recovered_documents_sri(query):\n",
        "  \"\"\"\n",
        "  Determines the set of documents recovered. The most important one is in position zero and thus the relevance decreases.\n",
        "\n",
        "  Args:\n",
        "    - query (str): Query text.\n",
        "\n",
        "  Return:\n",
        "    list: List of document identifiers\n",
        "\n",
        "  \"\"\"\n",
        "  document_identifiers = [t[0] for t in dataset.docs_iter()]\n",
        "  random.shuffle(document_identifiers)\n",
        "  recovered_documents = document_identifiers[:random.randint(1, len(document_identifiers) - 1)]\n",
        "  return recovered_documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T49wzOHjQ8tW"
      },
      "source": [
        "Luego, se requiere que implemente cada una de las métricas que a continiación se definen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Hpk6SYNDrV"
      },
      "outputs": [],
      "source": [
        "def precision(recovered_documents, relevant_documents):\n",
        "  \"\"\"\n",
        "  Calculate the measure (accuracy)\n",
        "\n",
        "  Args:\n",
        "    - recovered_documents (list): Set of documents recovered by the SRI. Each document is defined by its identifier.\n",
        "    - relevant_documents (list): Set of relevant documents. Each document is defined by its identifier.\n",
        "\n",
        "  Return:\n",
        "    double: Value between 0 and 1.\n",
        "\n",
        "  \"\"\"\n",
        "  #! Not Implemented\n",
        "\n",
        "  return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee3dMlDyNNC0"
      },
      "outputs": [],
      "source": [
        "def recall(recovered_documents, relevant_documents):\n",
        "  \"\"\"\n",
        "  Calculate the measure\n",
        "\n",
        "  Args:\n",
        "    - recovered_documents (list): Set of documents recovered by the SRI. Each document is defined by its identifier.\n",
        "    - relevant_documents (list): Set of relevant documents. Each document is defined by its identifier.\n",
        "​\n",
        "  Return:\n",
        "    double: Value between 0 and 1.\n",
        "\n",
        "  \"\"\"\n",
        "  #! Not Implemented\n",
        "\n",
        "  return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3pkYBgCTYy5"
      },
      "outputs": [],
      "source": [
        "def f(recovered_documents, relevant_documents):\n",
        "  \"\"\"\n",
        "  Calculate the measure\n",
        "\n",
        "  Args:\n",
        "    - recovered_documents (list): Set of documents recovered by the SRI. Each document is defined by its identifier.\n",
        "    - relevant_documents (list): Set of relevant documents. Each document is defined by its identifier.\n",
        "​\n",
        "  Return:\n",
        "    double: Value between 0 and 1.\n",
        "\n",
        "  \"\"\"\n",
        "  #! Not Implemented\n",
        "\n",
        "  return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaOJhwZNZJs9"
      },
      "outputs": [],
      "source": [
        "def f1(recovered_documents, relevant_documents):\n",
        "  \"\"\"\n",
        "  Calculate the measure\n",
        "\n",
        "  Args:\n",
        "    - recovered_documents (list): Set of documents recovered by the SRI. Each document is defined by its identifier.\n",
        "    - relevant_documents (list): Set of relevant documents. Each document is defined by its identifier.\n",
        "​\n",
        "  Return:\n",
        "    double: Value between 0 and 1.\n",
        "\n",
        "  \"\"\"\n",
        "  #! Not Implemented\n",
        "\n",
        "  return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4xBmUGRTeQU"
      },
      "outputs": [],
      "source": [
        "def r_precicion(recovered_documents, relevant_documents, r):\n",
        "  \"\"\"\n",
        "  Calculate the measure\n",
        "\n",
        "  Args:\n",
        "    - recovered_documents (list): Set of documents recovered by the SRI. Each document is defined by its identifier.\n",
        "    - relevant_documents (list): Set of relevant documents. Each document is defined by its identifier.\n",
        "    - r (int): Ranking position to apply the cut​\n",
        "\n",
        "  Return:\n",
        "    double: Value between 0 and 1.\n",
        "\n",
        "  \"\"\"\n",
        "  #! Not Implemented\n",
        "\n",
        "  return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZBTyYZ3T3F_"
      },
      "outputs": [],
      "source": [
        "def fallout(recovered_documents, relevant_documents, r):\n",
        "  \"\"\"\n",
        "  Calculate the measure\n",
        "\n",
        "  Args:\n",
        "    - recovered_documents (list): Set of documents recovered by the SRI. Each document is defined by its identifier.\n",
        "    - relevant_documents (list): Set of relevant documents. Each document is defined by its identifier.\n",
        "    - r (int): Ranking position to apply the cut​\n",
        "\n",
        "  Return:\n",
        "    double: Value between 0 and 1.\n",
        "\n",
        "  \"\"\"\n",
        "  #! Not Implemented\n",
        "\n",
        "  return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNny4kBGUDON"
      },
      "source": [
        "Para poder verificar las funciones, tome una consulta y determine por su sistema el conjunto de documentos recuperados. Luego, utilice la función siguiente para el apoyo visual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_aO13UeVlZI"
      },
      "outputs": [],
      "source": [
        "relevant_documents, query_text = relevant_documents('18')\n",
        "recovered_documents = recovered_documents_sri(query_text)\n",
        "\n",
        "print(f\"\"\"\n",
        "Identificador de la consulta: {query_id}\n",
        "Consulta: {query_text}\n",
        "\n",
        "Métricas:\n",
        "  Precisión: {precision(recovered_documents, relevant_documents)}\n",
        "  Recobrado: {recall(recovered_documents, relevant_documents)}\n",
        "  F: {f(recovered_documents, relevant_documents)}\n",
        "  F1: {f1(recovered_documents, relevant_documents)}\n",
        "  R-Precisión: {r_precicion(recovered_documents, relevant_documents, r)}\n",
        "  Fallout: {fallout(recovered_documents, relevant_documents, r)}\n",
        "\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
