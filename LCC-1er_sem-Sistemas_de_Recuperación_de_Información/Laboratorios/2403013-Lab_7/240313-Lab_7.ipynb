{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalando las dependencias ...\n",
        "\n",
        "Durante la clase se estarán utilizando las siguientes bibliotecas:\n",
        "- `requests` permite enviar solicitudes HTTP fácilmente.\n",
        "- `beautifulsoup4` facilita la extracción de información sintáctica de páginas web analizando el HTML.\n",
        "  \n",
        "Compruebe que tenga las bibliotecas o instálelas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4 requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Laboratorio #7: Web Crawling y Web Scraping\n",
        "\n",
        "Web crawling y Web scraping son dos términos relacionados pero distintos en el ámbito de la extracción de datos en la Web. \n",
        "\n",
        "**Web Crawling** (Rastreo web):\n",
        "- Proceso automatizado de explorar la Web y recopilar información de diversas páginas web.\n",
        "- Los programas son conocidos como \"arañas\" o \"rastreadores web\" y navegan por la web siguiendo enlaces de una página a otra.\n",
        "- Tiene como objetivo principal indexar el contenido de la web para los motores de búsqueda, de forma que los motores de búsqueda puedan recopilar datos sobre páginas web y actualizar sus índices.\n",
        "- Es un proceso más amplio que el scraping porque se enfoca en recorrer y descubrir nuevas páginas, en lugar de extraer datos específicos de las páginas.\n",
        "  \n",
        "**Web Scraping** (Extracción web):\n",
        "- Proceso de extracción de datos específicos de páginas web.\n",
        "- Se utiliza para recopilar información estructurada de una página web como: texto, imágenes, enlaces, precios de productos, documentos, enlaces, etc.\n",
        "- A diferencia del rastreo web, este se centra en extraer información de páginas web específicas.\n",
        "- Se utiliza en una variedad de casos de uso como la recopilación de datos para análisis, la monitorización de precios en línea, la investigación de mercado, entre otros.\n",
        "\n",
        "Esta clase se enfocará en esos dos temas. Para ello tiene la siguiente problemática:\n",
        "> Los profesores de la asignatura de SRI han decidido fundar una Mipyme para vender peluches de Pokemon porque se han percatado que MatCom es un nicho sin explotar. Como es un negocio emergente, se ha decido comprar los peluches en Inglaterra. Los precios de los peluches de Pokemon fluctúan en el mercado y se requiere recopilar datos para confeccionar estadísticas para asegurar y maximizar las ganancias de los profesores. \n",
        ">\n",
        "> \n",
        "### Ejercicio #1:\n",
        "Construya una tabla con los datos de cada Pokemon. Puede tomar la información de `https://scrapeme.live/shop/`. De cada peluche se quiere recuperar: nombre del Pokemon, precio en libras esterlinas, descripción, url donde está alojada una foto del Pokemon, url de donde se tomaron los datos.\n",
        "Recuerde que la empresa es nueva por lo que tiene pocos recursos. Por tanto, es requerimiento:\n",
        "- no visitar una página visitada anteriormente.\n",
        "- no consultar páginas web fuera de la tienda.\n",
        "- proceso solo se activará de forma manual.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os.path import join\n",
        "import csv\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pista #1:\n",
        "Culmine de implementar la clase `MyQueue`, la cual definirá una política de ordenación de las URLs.\n",
        "\n",
        "*¿Qué es la política de ordenación de URL?*\n",
        "\n",
        "*¿Qué le aporta al crawler?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyQueue:\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize a new instance\n",
        "        \n",
        "        \"\"\"\n",
        "        raise Exception('Not Implemented')\n",
        "        \n",
        "    def pop(self):\n",
        "        \"\"\"\n",
        "        Gets and returns an element\n",
        "\n",
        "        Return:\n",
        "            - t\n",
        "            \n",
        "        \"\"\"\n",
        "        raise Exception('Not Implemented')\n",
        "        \n",
        "    def add(self, elem):\n",
        "        \"\"\"\n",
        "        Adds an element to the queue\n",
        "\n",
        "        Arg:\n",
        "            - elem (t) : Element to add.\n",
        "            \n",
        "        \"\"\"\n",
        "        raise Exception('Not Implemented')\n",
        "            \n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of items stored\n",
        "\n",
        "        Return:\n",
        "            - int\n",
        "            \n",
        "        \"\"\"\n",
        "        return len(self._queue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Función de regalo para almacenar datos en un fichero CSV. Luego de culminarse la clase, los profesores aprovecharán los datos recopilados para comenzar la inversión. \n",
        "# 😈💰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def write_to_csv(data, path='.', file_name='pokemon-products'):\n",
        "    \"\"\"\n",
        "    Write data to a CSV file\n",
        "\n",
        "    Arg:\n",
        "        - products (list<dict>) : List of dictionaries. It is assumed that each dictionary has the same keys.\n",
        "\n",
        "    \"\"\"\n",
        "    with open(join(path, file_name + '.csv'), 'w', newline='', encoding='utf-8') as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "\n",
        "        # Escribir encabezados (suponiendo que todos los productos tienen las mismas claves)\n",
        "        if products:\n",
        "            writer.writerow(products[0].keys())\n",
        "\n",
        "        # Escribir los valores de los productos en el archivo CSV\n",
        "        for product in products:\n",
        "            writer.writerow(product.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lista para almacenar los datos extraídos de los productos de Pokemon\n",
        "products = []\n",
        "\n",
        "# URL base de la página web a scrapear\n",
        "base_url = 'https://scrapeme.live/shop/'\n",
        "\n",
        "# Inicialización de una cola para almacenar las URLs a visitar\n",
        "urls = MyQueue()\n",
        "urls.add('https://scrapeme.live/shop/') # Agregar la primera URL a la cola: conjunto semilla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La celda siguiente es para que implemente el proceso de recaudación de datos.\n",
        "\n",
        "### Pista #2.x\n",
        "1. Utilice la clase `MyQueue` para almacenar los hipervínculos extraídos de cada sitio web y extraer la página web a visitar.\n",
        "   \n",
        "2. La función `requests.get/1` dada una url, obtiene todo el código HTML de la página. Para obtener ese código basta consultarle la propiedad `content` a lo que retorna.\n",
        "   \n",
        "3. Utilizando `BeautifulSoup(contenido_html, 'html.parser')` obtienes un objeto cómodo para consultar el eodigo HTML cutilizando etiquetas y clases específicas.\n",
        "   \n",
        "4. Para obtener los enlaces de la página web puede hacer `soup.select('a[href]')`. De cada elemento que devuelva ese método, si desea manejar directamente el hipervínculo, indexe el objeto en `'href'`.\n",
        "   \n",
        "5. Para obtener el nombre del Pokemon puede hacer `soup.find('h1', class_='product_title entry-title').text`. \n",
        "   \n",
        "6. Para obtener el precio del peluche puede hacer `soup.find('p', class_='price').text.strip()`.\n",
        "   \n",
        "7. Para obtener la descripción del juguete puede hacer `soup.find('div', class_='woocommerce-product-details__short-description').text.strip()`.\n",
        "    \n",
        "8.  Para obtener la imagen del Pokemon puede hacer `soup.select_one('.wp-post-image')['src']`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pista #3:\n",
        "Se recomienda:\n",
        "- Discutir el flujo del proceso antes de implementarlo.\n",
        "- Detener el proceso cuando `products` tenga 10 elementos porque la Mipyme aun no ha dado recompensas y los directivos no pagarán el uso de los datos móviles particulares."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raise Exception('Not Implemented')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Escribir los productos extraídos en un archivo CSV\n",
        "write_to_csv(products)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio #2:\n",
        "a) ¿Cómo modificar el algoritmo para implementar la política de amabilidad? ¿Qué regla(s) implementarías?\n",
        "\n",
        "b) ¿Tiene sentido implementar alguna política de revisitado? De ser positiva la respuesta, ¿cuál sería?\n",
        "\n",
        "c) ¿La solución propuesta le otorga prioridad a las URLs para visitarlas?\n",
        "\n",
        "d) ¿La solución propuesta contempla una demora entre cada pedido o consulta? De ser negativa la respuesta y teniendo en cuenta que eso implica que una carga excesiva para el servidos y puede bloquear el IP del proceso en el cual se ejecuta el crawler, ¿qué puede hacer para controlar esto?\n",
        "\n",
        "e) ¿Existe algún mecanismo para extraer la información en un menor tiempo? ¿Qué problemas pueden crearse?\n",
        "\n",
        "f) ¿El crawler es persistente y robusto?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Existe herramientas que permiten hacer estos procesos. Estas son:\n",
        "- ZenRows: \n",
        "  - API integral de rastreo y raspado. \n",
        "  - Ofrece proxies rotativos, geolocalización, renderizado de JavaScript y bypass antibloqueo avanzado.\n",
        "- Scrapy: \n",
        "  - Opción de biblioteca de rastreo en Python, muy poderosa para principiantes. \n",
        "  - Proporciona un marco de alto nivel para crear rastreadores escalables y eficientes.\n",
        "- Selenium: \n",
        "  - Biblioteca popular de navegador para rastreo y raspado web. \n",
        "  - A diferencia de BeautifulSoup, puede interactuar con páginas web en un navegador como lo harían los usuarios humanos.\n",
        "  - Está implementada para disímiles lenguajes de programación."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
