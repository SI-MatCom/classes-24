{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalando las dependencias ...\n",
        "\n",
        "Durante la clase se estarán utilizando las siguientes bibliotecas:\n",
        "- `librosa` para el procesamiento de sonidos.\n",
        "- `matplotlib` para la visualización de datos y gráficos.\n",
        "- `scikit-learn` para el aprendizaje automático y las métricas de evaluación del modelo.\n",
        "- `pydub` y `PyObjC` para reproducir sonidos.\n",
        "\n",
        "Compruebe que tenga las bibliotecas o instálelas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eTQ5UAqYz8v"
      },
      "outputs": [],
      "source": [
        "!pip3.10 install librosa matplotlib scikit-learn playsound PyObjC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Laboratorio #6: Procesamiento de señales (sonidos específicamente)\n",
        "\n",
        "El procesamiento de sonidos desempeña un papel fundamental en los Sistemas de Recuperación de Información (SRI). Por ejemplo:\n",
        "\n",
        "- Indexación y búsqueda eficientes: El procesamiento de sonidos permite extraer características relevantes de los archivos de audio para indexar y buscar eficientemente dentro de grandes conjuntos de datos. \n",
        "- Mejora de la precisión en la recuperación: Al analizar el contenido sonoro de los archivos, los SRI pueden \"entender\" mejor el contexto y el contenido de los archivos de audio. Esto puede mejorar la precisión de las consultas y devolver resultados más relevantes a los usuarios.\n",
        "- Diversificación de los tipos de datos: Con el procesamiento de sonido, los SRI pueden manejar una variedad más amplia de tipos de datos. Esto significa que pueden indexar y recuperar no solo texto, imágenes o videos, sino también archivos de audio, lo que amplía las posibilidades de búsqueda y la experiencia del usuario.\n",
        "- Aplicaciones prácticas: Los SRI basados en el procesamiento de sonido tienen numerosas aplicaciones prácticas, que van desde la búsqueda de música y podcasts hasta la transcripción automática de archivos de audio y la recuperación de información en entornos complejos de multimedia.\n",
        "\n",
        "En resumen, el procesamiento de sonidos en los sistemas de recuperación de información es esencial para entender y organizar el contenido sonoro.\n",
        "\n",
        "Durante la clase se usará el conjunto de datos `ESC10-rearranged` el cual es una versión modificada `ESC-10`. `ESC-10` es un conjunto de datos de audio utilizado comúnmente en tareas de reconocimiento de sonidos ambientales. Contiene grabaciones de sonidos ambientales de diez clases diferentes, con cada clase representando un tipo específico de sonido ambiental.\n",
        "\n",
        "Las diez clases son las siguientes:\n",
        "- Ladrido de perro (Dog bark)\n",
        "- Lluvia (Rain)\n",
        "- Olas del mar (Sea waves)\n",
        "- Llanto de bebé (Baby cry)\n",
        "- Tic tac de un reloj (Clock tick)\n",
        "- Estornudo de persona (Person sneeze)\n",
        "- Helicóptero (Helicopter)\n",
        "- Motosierra (Chainsaw)\n",
        "- Gallo (Rooster)\n",
        "- Crepitación del fuego (Fire crackling)\n",
        "  \n",
        "El objetivo de la clase es visualizar y definir extractores de características de sonidos. Además, teniendo distintas representaciones se evaluará la que mejor describe al conjunto de datos para la clasificación de las diez categorías presentadas anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargando las bibliotecas necesarias para trabajar ...\n",
        "\n",
        "import os\n",
        "import random\n",
        "from os.path import join\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from playsound import playsound\n",
        "\n",
        "from classifier import classifier as teacher_classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio #1: Cargar las señales y otros datos en memoria.\n",
        "\n",
        "Conociendo que cada carpeta contiene la **clasificación** de los archivos de sonido, almacene en un diccionario la información. El diccionario está definido como `<str, list<(np.narray, int, str)>>` donde la llave del diccionario es la clasificación y cada tupla de la lista tiene:\n",
        "- archivo de sonido cargado en memoria.\n",
        "- cantidad de muestras tomadas (re-muestro, resampling)\n",
        "- ruta del fichero\n",
        "\n",
        "Para cargar el fichero de sonido considere usar la función `librosa.load`. \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(root_folder='dataset'):\n",
        "    \"\"\"\n",
        "    Load the data\n",
        "    \n",
        "    Arg:\n",
        "        - root_folder (str) : Path of the folder containing the data.\n",
        "        \n",
        "    Return:\n",
        "        - dict<str, list<(np.narray, int, str)>>\n",
        "    \n",
        "    \"\"\"\n",
        "    raise Exception(\"Not Implemented\")\n",
        "        \n",
        "dataset = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mostremos la representación de la onda y su espectrograma de señal tomada al alzar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seleccionar aleatoriamente una clave del diccionario 'dataset'\n",
        "random_key = random.choice(list(dataset.keys()))\n",
        "\n",
        "# Tomar la señal a graficar\n",
        "audio, sr, file = random.choice(dataset[random_key])\n",
        "\n",
        "# Calcular el tiempo correspondiente a cada muestra de la señal de audio. Necesario para el eje x\n",
        "time = np.arange(0, len(audio)) / sr\n",
        "\n",
        "# Imprimir el nombre del archivo seleccionado aleatoriamente. En caso de querer buscarlo y oir\n",
        "print('Fichero: ', file)\n",
        "\n",
        "# Graficar la forma de onda del audio\n",
        "plt.figure(figsize=(12, 6)) \n",
        "plt.plot(time, audio) \n",
        "plt.title('Forma de onda del audio') \n",
        "plt.xlabel('Tiempo (segundos)') \n",
        "plt.ylabel('Amplitud') \n",
        "\n",
        "# Calcular el espectrograma del audio y visualizarlo\n",
        "D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max) \n",
        "plt.figure(figsize=(12, 6)) \n",
        "librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log') \n",
        "plt.colorbar(format='%+2.0f dB') \n",
        "plt.title('Espectrograma del audio') \n",
        "\n",
        "# playsound(file)\n",
        "\n",
        "# Mostrar los gráficos\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preguntas previas:\n",
        "1. ¿Qué representa la escala de Mel? ¿Para qué se utiliza?\n",
        "2. ¿Qué representa un Cent? ¿En dónde se ha visto el uso de esta escala?\n",
        "3. ¿Qué información captura los estractores de características que se definen en el ejercicio siguiente?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio #2: Implemente extractores de características de los sonidos.\n",
        "\n",
        "Se recomienda trabajar con los siguientes extractores:\n",
        "- Coeficientes ceptrales de frecuencia Mel\n",
        "- Espectograma de Mel (similar al espectrograma tradicional pero las frecuencias se mapean a la escala de Mel)\n",
        "- Centroide espectral \n",
        "- Cens de la señal\n",
        "\n",
        "Se recomienda hacer uso del módulo `librosa.feature`. De usarse, es importante considerar trabajar tanto con la señal como con el cantidad de muestras tomadas en el re-muestreo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# raise Exception(\"Not Implemented\")\n",
        "mfccs_extractor = lambda audio, sr: None\n",
        "mel_spectrum_extractor = lambda audio, sr: None\n",
        "spectral_centroid_extractor = lambda audio, sr: None\n",
        "cens_extractor = lambda audio, sr: None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definidas las funciones para extraer las características, carguemos el mismo ejemplo tomado aleatoriamente y mostremos su representación visual de cada extractor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mfccs = mfccs_extractor(audio, sr)\n",
        "\n",
        "# Mostrar los MFCCs\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(mfccs, x_axis='time')\n",
        "plt.colorbar()\n",
        "plt.title('MFCCs')\n",
        "plt.xlabel('Tiempo')\n",
        "plt.ylabel('Coeficientes de Mel')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mel_spectrum = mel_spectrum_extractor(audio, sr)\n",
        "\n",
        "# Visualizar el espectrograma de Mel\n",
        "plt.figure(figsize=(10, 6))\n",
        "librosa.display.specshow(mel_spectrum, sr=sr, x_axis='time', y_axis='mel')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Espectrograma de Mel')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spectral_centroid = spectral_centroid_extractor(audio, sr)\n",
        "\n",
        "# Visualizar el centroide espectral a lo largo del tiempo\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogy(spectral_centroid.T, label='Centroide espectral')\n",
        "plt.ylabel('Hz')\n",
        "plt.xticks([])\n",
        "plt.xlim([0, spectral_centroid.shape[-1]])\n",
        "plt.legend()\n",
        "plt.title('Centroide espectral')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cens = cens_extractor(audio, sr)\n",
        "\n",
        "# Visualizar el cens\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(cens, y_axis='chroma', x_axis='time')\n",
        "plt.colorbar()\n",
        "plt.title('Chroma CENS')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La posibilidad de contar con distintos extractores y con ellos distintas representaciones asociadas a una misma señal, nos facilita determinar la \"mejor\" representación según las tareas que tendrá nuestro SRI.\n",
        "\n",
        "### Ejercicio #3: Analice y discuta la mejor representación implementada de la señal para clasificarlas en las 10 categorías mencionadas al inicio de la clase\n",
        "\n",
        "Para ello, apóyese de la función `teacher_classifier`. Además, antes de utilizarla debe de volcar toda la información almacenada en la variable `dataset` en dos variables que serán los datos y las categorías de cada uno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "raise Exception(\"Not Implemented\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('mfccs_extractor\\n', teacher_classifier(mfccs_extractor, data, labels))\n",
        "print('\\nmel_spectrum_extractor\\n', teacher_classifier(mel_spectrum_extractor, data, labels))\n",
        "print('\\ncens_extractor\\n', teacher_classifier(cens_extractor, data, labels))\n",
        "print('\\nspectral_centroid_extractor\\n', teacher_classifier(spectral_centroid_extractor, data, labels))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
