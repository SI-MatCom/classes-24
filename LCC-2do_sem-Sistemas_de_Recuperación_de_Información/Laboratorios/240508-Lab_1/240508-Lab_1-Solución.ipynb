{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio#1: Modelo Booleano de Recuperación de Información\n",
    "\n",
    "Un Modelo de Recuperación de Información (MRI) es un sistema diseñado para facilitar la búsqueda y extracción de información relevante de grandes bases de datos o colecciones de documentos. Estos modelos son cruciales en áreas como los motores de búsqueda en internet, los sistemas de gestión documental y las bibliotecas digitales. Permiten a los usuarios encontrar información específica basada en criterios de búsqueda variados. Algunos de los modelos más significativos incluyen el modelo booleano, que utiliza operadores lógicos para la búsqueda; y el modelo vectorial, que emplea conceptos de álgebra lineal para relacionar documentos y consultas.\n",
    "\n",
    "Durante el laboratorio se estará trabajando con el Modelo Booleano. Este de define como:\n",
    "- **D**: Vectores binarios asociados a los términos indexados.\n",
    "- **Q**: Expresión booleana sobre los términos indexados, utilizando los operadores: _and_, _or_, _not_.\n",
    "- **F**: Álgebra booleana y teoría de conjuntos.\n",
    "- **R**: $sim(d_j, q) = \n",
    "\t\t \t\t\\left\\{\n",
    "\t\t \t\t\t\\begin{array}{ll} \n",
    "\t\t \t\t\t\t1 & si \\ \\exists \\overrightarrow{q_{cc}} : \\overrightarrow{q_{cc}} \\in \\overrightarrow{q_{fnd}} \\ \\wedge \\ (\\forall t_i :\\  g_i(\\overrightarrow{d_j}) = g_i(\\overrightarrow{q_{cc}}))\\\\ \\\\\n",
    "\t\t \t\t\t\t0 & en \\ otro \\ caso\n",
    "\t\t \t\t\t\\end{array} \n",
    "\t\t \t\t\\right.\n",
    "\t\t \t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando el entorno\n",
    "\n",
    "# Fuente del corpus\n",
    "import ir_datasets\n",
    "\n",
    "# Trabajo con los operadores lógicos\n",
    "from sympy import sympify, to_dnf, Not, And, Or\n",
    "\n",
    "# Facilita el trabajo con los términos indexados del corpus\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Funciones útiles auxiliares\n",
    "from teacher_help import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cranfield\n",
    "\n",
    "El corpus Cranfield es un conjunto clásico de datos en el campo de la Recuperación de Información compuesto por aproximadamente 1,400 resúmenes de artículos de investigación en aerodinámica. Cada documento en este corpus incluye un título, un resumen conciso del contenido, y en algunos casos, palabras clave y referencias bibliográficas. Acompañando a estos documentos, hay alrededor de 225 consultas de prueba y sus correspondientes juicios de relevancia, proporcionados por expertos, que indican la pertinencia de cada documento para una consulta específica. Este diseño estructurado y su enfoque específico en temas de aerodinámica hacen del corpus una herramienta esencial para evaluar la eficacia de Sistemas de Recuperación de Información (SRI), sirviendo como un modelo estándar para pruebas comparativas y consistentes en esta disciplina.\n",
    "\n",
    "La variable **dataset**, instancia de la clase **ir_datasets.datasets.base.Dataset**, tiene 3 funciones:\n",
    "1. _docs_iter()_\n",
    "2. _queries_iter()_\n",
    "3. _qrels_iter()_\n",
    "\n",
    "Pero durante esta clase, solo se trabajará con la primera de ellas. La función **docs_iter()** devuelve un objeto iterable de tuplas de dimensión 5, referentes a los documentos. Los campos de cada tupla son:\n",
    "  - Identificador (str)\n",
    "  - Título (str)\n",
    "  - Texto (str)\n",
    "  - Autor (str)\n",
    "  - Referencia bibliográfica (str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio #1: Cargue en memoria el corpus Cranfield y tokenice cada documentos.\n",
    "###### Ayuda: Consulte la función `tokenize/1` hecha por los profesores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception('Not Implemented')\n",
    "\n",
    "dataset = ir_datasets.load(\"cranfield\")\n",
    "tokenized_docs = [(doc[0], tokenize(doc[2], ['and', 'or', 'not', '(', ')', '&', '~', '|'])) for doc in dataset.docs_iter()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando mi primera estructura de datos para un MRI\n",
    "\n",
    "Los términos invertidos, también conocidos como **índices invertidos**, son estructuras de datos utilizadas en Sistemas de Recuperación de Información (SRI) para asociar términos (o palabras) con los documentos que los contienen. Básicamente, son diccionarios que asignan términos a una lista de documentos en los que aparecen esos términos. Esto permite una búsqueda eficiente de documentos que contienen ciertos términos.\n",
    " \n",
    "\n",
    "### Ejercicio #2: Implemente una función que dado un corpus tokenizado, devuelva los índices invertidos.\n",
    "Considere que cada documento posee un identificador único."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(data):\n",
    "    \"\"\"\n",
    "    Build an inverted index structure\n",
    "    \n",
    "    Args:\n",
    "    - data : [(str, [str])]\n",
    "        List of tuples of size 2. The first position contains the document identifier and the second the list of document tokens.\n",
    "\n",
    "    Return:\n",
    "    - dict\n",
    "    \n",
    "    \"\"\"\n",
    "    # raise Exception('Not Implemented')\n",
    "    \n",
    "    inverted_index = {}\n",
    "    \n",
    "    for doc_id, document in data:\n",
    "        for word in document:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = set()\n",
    "            inverted_index[word].add(doc_id)\n",
    "                \n",
    "    return inverted_index\n",
    "\n",
    "build_inverted_index(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación de la consulta en FND\n",
    "\n",
    "La Forma Normal Disyuntiva es una forma estándar de expresar una fórmula lógica booleana como una disyunción de cláusulas, donde cada cláusula es una conjunción de literales (variables booleanas o sus negaciones). Esta representación es útil en la lógica booleana y en la teoría de la computación porque permite realizar operaciones y simplificaciones sobre las expresiones booleanas de manera sistemática. Además, muchas funciones lógicas pueden ser representadas eficientemente en DNF.\n",
    "\n",
    "### Ejercicio #3: Implemente una función que dada una query booleana, devuelva la FND de la misma.\n",
    "###### Ayuda #1: Considere usar las funciones `sympify/1` y `to_dnf/2`.\n",
    "###### Ayuda #2: Considere usar los símbolos `&`, `|` y `~` para sustituir los operadores lógicos `and`, `or` y `not`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_dnf(query):\n",
    "    \"\"\"\n",
    "    Transforms a boolean expression into its disjunctive normal form\n",
    "    \n",
    "    Args:\n",
    "    - query : str\n",
    "        Boolean query.\n",
    "\n",
    "    Return:\n",
    "    - instance of class Or\n",
    "    \n",
    "    \"\"\"\n",
    "    # raise Exception('Not Implemented')\n",
    "    \n",
    "    query = query.lower().replace(\"and\", \"&\").replace(\"or\", \"|\").replace(\"not\", \"~\")\n",
    "\n",
    "    # Procesar la consulta con spaCy\n",
    "    tokens = tokenize(query)\n",
    "\n",
    "    # Añadir '&' donde sea necesario (convenio)\n",
    "    processed_tokens = []\n",
    "    for i in range(len(tokens)):\n",
    "        processed_tokens.append(tokens[i])\n",
    "        if i < len(tokens) - 1 and tokens[i] not in ['(', '&', '~', '|'] and tokens[i+1] not in [')', '&', '~', '|']:\n",
    "            processed_tokens.append('&')\n",
    "\n",
    "    # Unir la cadena\n",
    "    processed_query = ' '.join(processed_tokens)\n",
    "    # Convertir a expresión sympy y aplicar to_dnf\n",
    "    query_expr = sympify(processed_query, evaluate=False)\n",
    "    query_dnf = to_dnf(query_expr, simplify=True)\n",
    "\n",
    "    return query_dnf\n",
    "\n",
    "query = \"A AND (B OR NOT C)\"\n",
    "query_dnf = query_to_dnf(query)\n",
    "print(query_dnf)\n",
    "# query_dnf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio #4: Implemente la función de similitud del modelo booleano, de forma que dada la consulta y el conjunto de documentos tokenizados la función devuelva aquellos documentos que satifacen a la consulta.\n",
    "###### Ayuda: Considere usar las funciones `Dictionary/1`, `Dictionary.token2id/0` y `Dictionary.doc2bow/1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_docs(query_dnf):\n",
    "    \"\"\"\n",
    "    Similarity function\n",
    "\n",
    "    Args:\n",
    "        - query_dnf : Instance of class Or\n",
    "        Query in FND\n",
    "\n",
    "    Return:\n",
    "    - [boolean]. each position matches a document in the corpus. If the value is True then the document satisfies the query, False otherwise.\n",
    "\n",
    "    \"\"\"\n",
    "    # raise Exception('Not Implemented')\n",
    "  \n",
    "    # Crear un diccionario a partir de los documentos tokenizados\n",
    "    dictionary = Dictionary([doc for _, doc in tokenized_docs])\n",
    "    # Obtener el vocabulario\n",
    "    vocabulary = list(dictionary.token2id.keys())\n",
    "\n",
    "    # Convertir los documentos tokenizados a representación de bolsa de palabras\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in [doc for _, doc in tokenized_docs]]\n",
    "\n",
    "    # Función para verificar si un documento satisface una componente conjuntiva de la consulta\n",
    "    def satisfies_conjunctive_component(doc_bow, conjunctive_component):\n",
    "        doc_terms = set(dict(doc_bow).keys())\n",
    "        \n",
    "        for term in conjunctive_component.args:\n",
    "            # Obtener el identificador del término en el diccionario\n",
    "            \n",
    "            if isinstance(term, Not):\n",
    "                term_id = dictionary.token2id.get(term.args[0].name, -1)\n",
    "                # Si el término está negado, debe estar ausente en el documento\n",
    "                if term_id in doc_terms:\n",
    "                    return False\n",
    "            else:\n",
    "                # Si el término no está negado, debe estar presente en el documento\n",
    "                term_id = dictionary.token2id.get(term.name, -1)\n",
    "                if term_id not in doc_terms:\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    matching_documents = []\n",
    "    for doc_id, doc_bow in enumerate(corpus):\n",
    "        # Revisar cada componente conjuntiva de la consulta DNF\n",
    "        if isinstance(query_dnf, And) or isinstance(query_dnf, Or):\n",
    "            # Si la consulta DNF tiene múltiples términos\n",
    "            if any(satisfies_conjunctive_component(doc_bow, component) for component in query_dnf.args):\n",
    "                matching_documents.append(doc_id)\n",
    "        else:\n",
    "            # Si la consulta DNF es un solo término o una única conjunción negada\n",
    "            if satisfies_conjunctive_component(doc_bow, query_dnf):\n",
    "                matching_documents.append(doc_id)\n",
    "\n",
    "    return matching_documents\n",
    "\n",
    "get_matching_docs(query_dnf)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio #5: ¿Cómo pudiese implementarse la función de similitud usando solamente la consulta en su FND y la estructura de los índices invertidos creada a partir del corpus procesado?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
