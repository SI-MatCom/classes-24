{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio#2: Modelo Vectorial de Recuperación de Información\n",
    "\n",
    "El Modelo  Vectorial de Recuperación de Información (MVRI) es un modelo algebraico para representar documentos de texto (o más generalmente, elementos) como vectores, de modo que la distancia entre vectores representa la relevancia entre los documentos. Es fundamental para una serie de operaciones de recuperación de información (RI), incluida la puntuación de documentos en una consulta, la clasificación de documentos y la agrupación de documentos.\n",
    "\n",
    "Durante la clase se estará trabajando con el Modelo Vectorial. Este de define como:\n",
    "- **D**:  Vectores de pesos no binarios asociados a los términos de los documentos.\n",
    "- **Q**: Vectores de pesos no binarios asociados a los términos de la consulta.\n",
    "- **F**: Espacio n-dimensional y operaciones entre vectores del Álgebra Lineal.\n",
    "- **R**: $sim(d_j, q) = \n",
    "\t\t \t\t\\displaystyle{\\frac{\\sum_{i=1}^{n} w_{i,j} \\times w_{i,q}}{\\sqrt{\\sum_{i=1}^{n} w_{i,j}^{2}}\\times \\sqrt{\\sum_{i=1}^{n} w_{i,q}^{2}}}}\n",
    "\t\t \t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando el entorno\n",
    "\n",
    "# Fuente del corpus\n",
    "import ir_datasets\n",
    "\n",
    "# Facilita el trabajo con los términos indexados del corpus\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Facilita la obtención de los valores tf-idf de los documentos \n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Para generar la matriz densa de tf-idf\n",
    "from gensim.matutils import corpus2dense\n",
    "\n",
    "# Para realizar el proceso de reduccion de dimensiones\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Para generar los clusters de documentos\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Funciones útiles auxiliares\n",
    "from teacher_help import tokenize, cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cranfield\n",
    "\n",
    "El corpus Cranfield es un conjunto clásico de datos en el campo de la Recuperación de Información compuesto por aproximadamente 1,400 resúmenes de artículos de investigación en aerodinámica. Cada documento en este corpus incluye un título, un resumen conciso del contenido, y en algunos casos, palabras clave y referencias bibliográficas. Acompañando a estos documentos, hay alrededor de 225 consultas de prueba y sus correspondientes juicios de relevancia, proporcionados por expertos, que indican la pertinencia de cada documento para una consulta específica. Este diseño estructurado y su enfoque específico en temas de aerodinámica hacen del corpus una herramienta esencial para evaluar la eficacia de Sistemas de Recuperación de Información (SRI), sirviendo como un modelo estándar para pruebas comparativas y consistentes en esta disciplina.\n",
    "\n",
    "La variable **dataset**, instancia de la clase **ir_datasets.datasets.base.Dataset**, tiene 3 funciones:\n",
    "1. _docs_iter()_\n",
    "2. _queries_iter()_\n",
    "3. _qrels_iter()_\n",
    "\n",
    "Pero durante esta clase, solo se trabajará con la primera de ellas. La función **docs_iter()** devuelve un objeto iterable de tuplas de dimensión 5, referentes a los documentos. Los campos de cada tupla son:\n",
    "  - Identificador (str)\n",
    "  - Título (str)\n",
    "  - Texto (str)\n",
    "  - Autor (str)\n",
    "  - Referencia bibliográfica (str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "En el laboratorio anterior se vio, entre otras cosas, el proceso de la tokenización de documentos, la generación del vocabulario y la representación de los documentos segun BoW (Bag Of Words, o bolsa de palabras). \n",
    "\n",
    "Nuestro primer objetivo es ejercitar los contenidos antes mencionados y preparar los datos para su posterior uso. \n",
    "\n",
    "### Ejercicio 1: Prepare los datos para poder trabajar.\n",
    "Para ello, \n",
    "\n",
    "a) Cree el vocabulario del conjunto de datos.\n",
    "\n",
    "b) Convierta a la representación **bolsa de palabras** cada documento.\n",
    "\n",
    "###### Ayuda: Considere usar la clase `Dictionary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "En el caso de la representación de los documentos en el Modelo Vectorial ae considera un modelo de espacio vectorial particular basado en la representación de una bolsa de palabras. Los documentos y consultas se representan como vectores.\n",
    "\n",
    "$d_j = (w_{1,j},w_{2,j},\\ldots,w_{n,j})$\n",
    "\n",
    "$q = (q_{1,q},w_{2,q},\\ldots,w_{n,q})$\n",
    "\n",
    "Cada dimensión corresponde a un término o token independiente. Si un término aparece en el documento, su valor en el vector es distinto de cero. Se han desarrollado varias formas diferentes de calcular estos valores, conocidos también como ponderaciones. Uno de los esquemas más conocidos es la ponderación tf-idf.\n",
    "\n",
    "El tf–idf es el producto de dos estadísticas: la frecuencia de los términos y la frecuencia inversa de los documentos\n",
    "\n",
    "\n",
    "### Ejercicio 2: Obtenga la representación tf-idf de cada elemento del corpus y de la consulta definida.\n",
    "###### Ayuda: Considere usar la clase `TfidfModel` y la función `corpus2dense`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = 'what similarity laws must be obeyed when constructing aeroelastic models of heated high speed aircraft .'\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicio 3: Obtenga el ranking (decreciente) de los documentos que _satisfacen_ a la consulta previamente definida.\n",
    "\n",
    "###### Ayuda: Considere usar la función `cosine_similarity`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(corpus_matriz, vector_query):\n",
    "    \"\"\"\n",
    "    Gets the similarity between the corpus and a query\n",
    "    \n",
    "    Args:\n",
    "    - corpus_matriz : [[float]]\n",
    "        tf-idf representation of the query. Each row is considered a document.\n",
    "    - vector_query : [float]\n",
    "        tf-idf representation of the query.\n",
    "        \n",
    "    Return:\n",
    "    - [(int, float)]\n",
    "\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "retrieve_documents(corpus_tfidf_dense, query_vector[0])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Uno de los problemas más comunes cuando se trabaja con datos muchas dimensiones es el conocido \"maldición de la dimensionalidad\". Este fenómeno entre otras cosas afecta nociones como distancia o similitud, haciendo que todos los objetos parezcan distantes y diferentes.\n",
    "\n",
    "Debido a esto, una de las ideas básicas para evitar estos problemas consiste en aplicar técnicas de reducción de dimensiones. Este proceso consiste en la transformación de datos de un espacio de alta dimensión a un espacio de baja dimensión, pero en este nuevo espacio se intenta conservar algunas propiedades significativas de los datos originales, idealmente cercanas a su dimensión intrínseca.\n",
    "\n",
    "Una de las principales técnicas lineales para la reducción de la dimensionalidad es el Análisis de Componentes Principales (PCA por sus siglas en inglés). Esta técnica realiza un mapeo lineal de los datos a un espacio de dimensiones inferiores de tal manera que se maximiza la varianza de los datos en la representación de dimensiones bajas.\n",
    "\n",
    "### Ejercicio 4: Reduzca las dimensiones del corpus.\n",
    "###### Ayuda: Considere usar la clase `PCA` y de ahí, las funciones `fit` y `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Índice de varianza\n",
    "variance = 0.90\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Un sistema de recuperación a menudo devuelve miles de documentos en respuesta a una consulta amplia, lo que dificulta a los usuarios navegar o identificar información relevante. Una de las técnicas utilizadas para segregar parte de corpus en el análisis de una consulta son los conocidos métodos de agrupación, los cuales consisten de forma automática en asociar los documentos en una lista de categorías significativas.\n",
    "\n",
    "Los algoritmos de agrupamiento en el análisis computacional de texto unifican esos datos en subconjuntos o grupos, asegurando que los datos de cada grupos son internamente coherentes o similares y  externamente diferentes. \n",
    "\n",
    "Cada grupo representa un conjunto de elementos similares entre ellos, esto se pueden utilizar para disminuir la cantidad de documentos a la hora de realizar el ranking. Para ello, se calcula la similitud de cada cluster con una consulta, para posteriormente solo utilizar aquellos documentos que pertenezcan a los grupos más similares a la consulta.\n",
    "\n",
    "La idea del ejercicio es utilizar el metodo KMeans para generar grupos de documentos similares, y posteriormente quedarse solo con los aquellos grupos mas cercanos a la query.\n",
    "\n",
    "# Ejercicio 5: Reduzca el espacio de búsqueda de los documentos a recuperar, trabajando con la nueva representación del corpus.\n",
    "\n",
    "###### Ayuda: Considere usar los centroides generados por la clase `KMeans`. De igual forma, utilice las funciones  de cada cluster para encontrar la similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros a utilizar\n",
    "\n",
    "# Número de cluster\n",
    "n_clusters = 5\n",
    "\n",
    "# Número para filtrar la similitud mínima entre la consulta y cada centroide \n",
    "alpha = 0.8 # Puede variar\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Ejercicio 6: Genere un nuevo ranking de documentos usando solamente aquellos que pertenezcan a los grupos seleccionados en el ejercicio anterior.\n",
    "###### Ayuda: Considere usar la función `KMeans.labels_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
