{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio #3: Expansión de consultas\n",
    "\n",
    "La expansión de consultas es una técnica importante en la recuperación de información y en los motores de búsqueda por varias razones:\n",
    "1. Mejora de la precisión y recobrado:\n",
    "   - Al incluir términos relacionados y sinónimos se puede mejorar las métricas de los resultados de búsqueda. Esto ayuda a capturar documentos relevantes que podrían haber sido ignorados con la consulta original.\n",
    "\n",
    "2. Reducción de ambigüedad:\n",
    "   - Las consultas iniciales a menudo son ambiguas o demasiado breves. Expandir la consulta puede aclarar la intención del usuario y reducir la ambigüedad, mejorando la relevancia de los resultados.\n",
    "\n",
    "3. Manejo de sinónimos y variaciones lingüísticas:\n",
    "   - Asegura que se encuentren documentos relevantes que utilizan diferentes términos para referirse al mismo concepto.\n",
    "\n",
    "4. Mejora la experiencia de usuario:\n",
    "   - Proporciona una mejor experiencia de usuario al devolver resultados más completos y relevantes, lo que puede aumentar la satisfacción y la confianza en el sistema de búsqueda.\n",
    "\n",
    "5. Optimización para motores de búsqueda:\n",
    "   - En el contexto del SEO (Search Engine Optimization), expandir consultas con palabras clave relacionadas puede mejorar la visibilidad de los contenidos en los motores de búsqueda, atrayendo más tráfico y mejorando el posicionamiento.\n",
    "\n",
    "Existen distintos tipos de métodos y técnicas para expandir la consulta efectuada por un usuario. Los más comunes son:\n",
    "\n",
    "1. Reformulación de la consula\n",
    "  - Sinónimos y términos relacionados: Uso de tesauros y/o diccionarios para encontrar sinónimos y términos relacionados que puedan añadirse a la consulta.\n",
    "  - Expansión manual: Pedirle a los usuarios que proporcionen términos adicionales o frases que consideren relevantes, a partir de la consulta definida.\n",
    "\n",
    "2. Retroalimentación\n",
    "  - Retroalimentación Relevante: Utiliza la retroalimentación de relevancia en la que los usuarios marcan documentos relevantes. Luego, ajusta la consulta para incluir términos encontrados en estos documentos.\n",
    "  - Pseudo-relevancia: Considera los primeros resultados de la búsqueda como relevantes y usa estos documentos para expandir la consulta.\n",
    "\n",
    "3. Modelos de Lenguaje y Procesamiento del Lenguaje Natural \n",
    "  - Modelos de lenguaje: Utiliza modelos BERT o GPT para sugerir términos relacionados y expandir la consulta.\n",
    "  - Word embeddings: Usa técnicas como Word2Vec o GloVe para encontrar términos semánticamente similares y añadirlos a la consulta.\n",
    "\n",
    "4. Expansión basada en la búsqueda\n",
    "  - Exploración de términos: Extraer términos importantes de los N primeros documentos recuperados inicialmente y usa estos términos para expandir la consulta.\n",
    "  - Consulta dividida: Divide la consulta en subconsultas más pequeñas y combina los resultados.\n",
    "\n",
    "\n",
    "Durante el laboratorio, se implementarán distintos métodos para expandir una consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando librerías necesarias\n",
    "\n",
    "# Fuente del corpus\n",
    "import ir_datasets\n",
    "\n",
    "# Útil para el trabajo con arreglo numéricos\n",
    "import numpy as np\n",
    "\n",
    "# Facilita el trabajo con los términos indexados del corpus\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Interfaz para el trabajo con Wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Útil para calcular la distancia de Levenshtein entre 2 palabras\n",
    "from Levenshtein import distance as distance_Levenshtein\n",
    "\n",
    "# Funciones útiles auxiliares\n",
    "from teacher_help import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio #1**: Implemente la expasión de consultas usando la matriz de co-ocurrencia.\n",
    "\n",
    "a) Explique cómo se construye la matriz co-ocurrencia.\n",
    "\n",
    "b) Implemente una función que dado un corpus (Cranfield) tokenizado, devuelva:\n",
    "- la matriz de co-ocurrencia normalizada (valores de 0 a 1),\n",
    "- un diccionario que permita dado un token, obtener un número único asociado al token, el cual es a su vez único dentro del vocabulario, y\n",
    "- un diccionario que permita dado un número, obtener el token asociado dentro del vocabulario.\n",
    "\n",
    "Considere utilizar solo los tokens \"más\" importantes para el análisis.\n",
    "\n",
    "c) Implemente una función que dada la matriz de co-ocurrencia y un token, devuelva aquellos tokens similares a partir del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise NotImplemented('Ejercicio 1.b - Cargando el corpus')\n",
    "\n",
    "corpus = ir_datasets.load(\"cranfield\")\n",
    "tokenized_corpus = [tokenize(doc[2], ['ADJ', 'NOUN', 'PROPN', 'VERB']) for doc in dataset.docs_iter()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(tokenized_corpus, window_size=float('inf')):\n",
    "    \"\"\"\n",
    "    Build the co-occurrence matrix from a corpus\n",
    "    \n",
    "    Args:\n",
    "    - tokenized_corpus : [[str]]\n",
    "        Tokenized corpus, each element represents a document.\n",
    "    - window_size : int\n",
    "        Window size to consider that 2 tokens occur. By default, the value `inf` is defined, which represents the same document as the window size.\n",
    "    \n",
    "    Returns:\n",
    "    - (A, B, C) where:\n",
    "        A np.ndarray represents the co-occurrence matrix (symmetric).\n",
    "        B {str: int} represents the encoding of each token in the vocabulary.\n",
    "        C {int: str} represents the token of each code.\n",
    "        \n",
    "    \"\"\"\n",
    "    # raise NotImplementedError(\"Ejercicio 1.b\")\n",
    "    \n",
    "    vocabulary = Dictionary(tokenized_corpus)\n",
    "    vocab_size = len(vocabulary)\n",
    "    \n",
    "    # Codifico cada documento, para no tener que hacerlo por cada token dentro del doble for \n",
    "    encode_corpus = [vocabulary.doc2idx(doc) for doc in tokenized_corpus]\n",
    "    \n",
    "    cooccurrence_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for encode_doc in encode_corpus:\n",
    "        for i, encode_word in enumerate(encode_doc):\n",
    "            start = max(i - window_size, 0)\n",
    "            end = min(i + window_size + 1, len(encode_doc))\n",
    "            \n",
    "            for j in encode_doc[start:end]:\n",
    "                if j != i:\n",
    "                    # Matriz simétrica\n",
    "                    cooccurrence_matrix[i][j] += 1\n",
    "                    cooccurrence_matrix[j][i] += 1\n",
    "    \n",
    "    normalized_matrix = (cooccurrence_matrix - cooccurrence_matrix.min()) / (cooccurrence_matrix.max() - cooccurrence_matrix.min())\n",
    "    return normalized_matrix, vocabulary.token2id, {id: word for word, id in vocabulary.token2id.items()}\n",
    "\n",
    "cooccurrence_matrix, token2id, id2token = build_cooccurrence_matrix(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_terms(token, cooccurrence_matrix, token2id, id2token, threshold=0.7, top_n=5):\n",
    "    \"\"\"\n",
    "    Returns terms close to a token\n",
    "    \n",
    "    Args:\n",
    "    - token : str\n",
    "        Token.\n",
    "    - cooccurrence_matrix : np.ndarray\n",
    "        Co-occurrence matrix, symmetric. The indices match the vocabulary encoding defined in `token2id`.\n",
    "    - token2id : {str: int}\n",
    "        Dictionary where the keys are the vocabulary tokens and the value is a unique numerical code.\n",
    "    - id2token : {int: str}\n",
    "        Dictionary where the keys are the code (unique numeric) and the value is a unique token within the vocabulary.\n",
    "    - threshold : float\n",
    "        Numeric value between 0 and 1, represents the minimum value that the token must have to be returned.\n",
    "    - top_n : int\n",
    "        Number of words to recover.\n",
    "        \n",
    "    Return:\n",
    "    - [str]\n",
    "    \n",
    "    \"\"\"\n",
    "    # raise NotImplementedError(\"Ejercicio 1.c\")\n",
    "    \n",
    "    if not token in token2id:\n",
    "        return []\n",
    "    \n",
    "    token_id = token2id[token]\n",
    "    words = [(value, id2token[pos]) for pos, value in enumerate(cooccurrence_matrix[token_id]) if pos != token_id and value >= threshold]\n",
    "    words = sorted(words, reverse=True)\n",
    "    return [t for _, t in words[:top_n]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'aerodynamic'\n",
    "\n",
    "# Consultar la ayuda de la función get_similar_terms/6 para ver el significado de los siguientes parámetros\n",
    "threshold = 0.3\n",
    "top_n = 10\n",
    "\n",
    "similar_terms = get_similar_terms(token, cooccurrence_matrix, token2id, id2token, threshold, top_n)\n",
    "\n",
    "print(f'Los tokens más similares a `{token}` utilizando matriz de co-ocurrencia son: \\n{similar_terms}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "En la práctica, si por ejemplo se tiene la consulta \n",
    "\n",
    "`Impacto del cambio climático en la biodiversidad marina`\n",
    "\n",
    "Luego de expandir la consulta, se obtiene:\n",
    "\n",
    "`cambio climático biodiversidad marina impacto efecto consecuencia política economía publicado:2015..2024`\n",
    "\n",
    "Es cierto que al sistema \"solo\" le interesa los tokens, sin importar el orden pero de esta manera se obvian algunos aspectos, como:\n",
    "- No se le da mayor relevancia a los tokens de la consulta original, teniendo todos la misma relevancia. Esto puede no ser cierto cuando se expande la consulta con términos muy generales o muy específicos, sacando al usuario de la zona de búsqueda esperada.\n",
    "- Puede existir una sobrecarga de información en la consulta expandida, conllevando a obtener resultados irrelevantes.\n",
    "- Aumenta la ambigüedad de la consulta.\n",
    "- No es trivial convertir la consulta en subconsultas.\n",
    "\n",
    "Por tanto, es conveniente definir una estructura donde se establezcan reglas, esto se conoce como **estructuración de la consulta**.\n",
    "\n",
    "Esta técnica se define a través de 3 operadores:\n",
    "1. Booleanos: Utiliza AND, OR y NOT para combinar términos.\n",
    "2. De agrupación: Utiliza paréntesis para agrupar términos.\n",
    "3. Para filtros específicos: Añade filtros a partir de un formato predefinido en el sistema.\n",
    "   \n",
    "Por tanto, la consulta \n",
    "\n",
    "`impacto del cambio climático en la biodiversidad marina`\n",
    "\n",
    "puede estructurarse de la siguiente manera:\n",
    "\n",
    "`(\"cambio climático\" AND \"biodiversidad marina\") AND (\"impacto\" OR \"efecto\" OR \"consecuencia\") NOT (\"política\" OR \"economía\") AND (publicado:2015..2024)` \n",
    "\n",
    "donde:\n",
    "- `(\"cambio climático\" AND \"biodiversidad marina\")`: Asegura que ambos términos \"cambio climático\" y \"biodiversidad marina\" aparezcan juntos.\n",
    "- `AND (\"impacto\" OR \"efecto\" OR \"consecuencia\")`: Añade términos relacionados con el impacto, aumentando la probabilidad de encontrar artículos relevantes. Al utilizar OR, ampliamos la búsqueda para incluir cualquiera de estos términos.\n",
    "- `NOT (\"política\" OR \"economía\")`: Excluye resultados relacionados con política o economía que pueden no ser relevantes para el enfoque científico o biológico deseado.\n",
    "- `AND (publicado:2015..2024)`: Añade un filtro de rango de fechas para obtener artículos publicados entre 2015 y 2024, asegurando que la información sea reciente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio #2:** Implemente una función que estructure una consulta.\n",
    "\n",
    "Para la realización de esta función, considere:\n",
    "- No trabajará con el operador NOT.\n",
    "- Todos los tokens identificados en la consulta original, tienen que aparecer en la consulta expandida.\n",
    "- Los tokens similares a añadir, será provisto de una función externa. Puede reutilizar la recién jecho con la matriz de co-ocurrencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_query(tokenized_query, get_similar_tokens_function):\n",
    "    \"\"\"\n",
    "    Structure a query\n",
    "\n",
    "    Args:\n",
    "    - tokenized_query : [str]\n",
    "        Tokenized query.\n",
    "    - get_similar_tokens_function : function\n",
    "        Function to obtain the tokens most similar to another. It is assumed to have cardinality 1.\n",
    "        \n",
    "    Return:\n",
    "    - str\n",
    "            \n",
    "    \"\"\"\n",
    "    # raise NotImplementedError('Ejercicio 2')\n",
    "\n",
    "    groups = [] \n",
    "\n",
    "    for token in tokenized_query:\n",
    "        similars = get_similar_tokens_function(token) \n",
    "        if similars:\n",
    "            group = [token] + similars\n",
    "            groups.append(f\"({' OR '.join(similars)})\")\n",
    "        else:\n",
    "            groups.append(token)\n",
    "\n",
    "    return ' AND '.join(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what similarity laws must be obeyed when constructing aeroelastic models of heated high speed aircraft.'\n",
    "tokenized_query = tokenize(query, ['ADJ', 'NOUN', 'PROPN', 'VERB'])\n",
    "                           \n",
    "# Imprimir la consulta original y la consulta estructurada\n",
    "print(f\"Consulta original: \\n{query}\")\n",
    "print(f\"\\nConsulta expandida y estructurada: \\n{structure_query(tokenized_query, lambda token: get_similar_terms(token, cooccurrence_matrix, token2id, id2token, threshold, top_n))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "La **distancia de Levenshtein**, también conocida como distancia de edición, es una métrica utilizada para medir la diferencia entre dos cadenas de texto. Específicamente, cuantifica el número mínimo de operaciones necesarias para transformar una cadena en otra. Las operaciones permitidas son:\n",
    "- Inserción: Añadir un carácter.\n",
    "- Eliminación: Quitar un carácter.\n",
    "- Sustitución: Cambiar un carácter por otro.\n",
    "\n",
    "Esta distancia es muy usada para:\n",
    "- sugerir correcciones ortográficas basadas en la similitud con palabras correctas. (Corrección Ortográfica)\n",
    "- ayudar a la identificación de patrones y coincidencias en el procesamiento de texto. (Reconocimiento de Patrones)\n",
    "- comparar secuencias de ADN. (Comparación de Cadenas de ADN)\n",
    "- la búsqueda difusa y la coincidencia aproximada de cadenas. (Procesamiento del Lenguaje Natural)\n",
    "\n",
    "Por su parte, es muy usual encontrarse con consultas donde sus términos presentan errores ortográficos. A consecuencia de esto, el sistema debe de ser capaz de detectar los errores ortográficos e identificar el token o la palabra por la cual debe de sustituirse.\n",
    "\n",
    "### Ejercicio #3: Construya una función que dada una consulta tokenizada, se arreglen los errores ortográficos presentes en la consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_spelling_errors(tokenized_query, vocabulary, threshold):\n",
    "    \"\"\"\n",
    "    Analyze a query and fix spelling errors present\n",
    "\n",
    "    Args:\n",
    "    - tokenized_query : [str]\n",
    "        Tokenized query.\n",
    "    - vocabulary: [str]\n",
    "        Vocabulary.\n",
    "    - threshold : int\n",
    "        Maximum number of changes to consider a word close to another with a spelling error. The value is assumed to be non-negative.\n",
    "\n",
    "    Return:\n",
    "    - [str]\n",
    "    \n",
    "    \"\"\"\n",
    "    def correct_word_func(word, vocabulary, threshold):\n",
    "        best_word = None\n",
    "        best_distance = float('inf')\n",
    "\n",
    "        for word_v in vocabulary:\n",
    "            distance = distance_Levenshtein(word, word_v)\n",
    "            if distance < best_distance:\n",
    "                best_distance = distance\n",
    "                best_word = word_v\n",
    "                \n",
    "        return best_word if best_distance <= threshold else None\n",
    "    \n",
    "    corrected_words = []\n",
    "    for word in tokenized_query:\n",
    "        correct_word = correct_word_func(word, vocabulary, threshold)\n",
    "        if correct_word:\n",
    "            corrected_words.append(correct_word)\n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kitten', 'sitting', 'apple', 'banana']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'whatt simmilarity laws msut be obayed wen consructing aeroelstic moddels of heateed high speeed aircaft.'\n",
    "tokenized_query = tokenize(query, ['ADJ', 'NOUN', 'PROPN', 'VERB'])\n",
    "\n",
    "vocabulary = None\n",
    "threshold = 100\n",
    "\n",
    "fix_spelling_errors(tokenized_query, vocabulary, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**WordNet** es una base de datos léxica del idioma inglés desarrollada por el Cognitive Science Laboratory de la Universidad de Princeton. Está organizada en grupos de términos conocidos como synsets (conjuntos de términos con relaciones), que representan conceptos específicos. Cada synset está vinculado a otros synsets a través de diversas relaciones semánticas y léxicas, como la hiponimia (relación entre términos más específicos y más generales) y la sinonimia (relación de sinónimos).\n",
    "\n",
    "WordNet se utiliza ampliamente en el procesamiento del lenguaje natural, la lingüística computacional y la inteligencia artificial debido a sus capacidades para:\n",
    "- Desambiguación de palabras.\n",
    "- Expansión de consultas.\n",
    "- Análisis semántico.\n",
    "- Generación de texto y traducción automática.\n",
    "- Desarrollo de aplicaciones de lenguaje.\n",
    "\n",
    "### **Ejercicio 4**: Utilizando WordNet, implemente una función para expandir una consulta. \n",
    "\n",
    "Esta función debe de hacer uso de los synsets, hipónimos e hiperónimos de cada término. Recuerde que:\n",
    "- Hipónimos: Son términos más específicos que la palabra dada (subclases).\n",
    "- Hiperónimos: Son términos más generales que la palabra dada (superclases).\n",
    "\n",
    "###### Ayuda: Considere usar las funciones: synsets/1, hyponyms/1, hypernyms/1 y lemmas/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_terms_wordnet(token):\n",
    "    \"\"\"\n",
    "    Returns the synsets, hypernyms, and hyponyms of a token\n",
    "\n",
    "    Args:\n",
    "    - token : str\n",
    "        Token.\n",
    "        \n",
    "    Return: \n",
    "    - dict\n",
    "        \n",
    "    \"\"\"\n",
    "    # raise NotImplementedError('Ejercicio 4')\n",
    "    \n",
    "    synset_set = set()\n",
    "    hyponyms_set = set()\n",
    "    hypernyms_set = set()\n",
    "    \n",
    "    for synset in wn.synsets(palabra):\n",
    "        for lemma in synset.lemmas():\n",
    "            synset_set.add(lemma.name())\n",
    "        \n",
    "        for hyponym in synset.hyponyms():\n",
    "            for lemma in hyponym.lemmas():\n",
    "                hyponyms_set.add(lemma.name())\n",
    "                \n",
    "        for hypernym in synset.hypernyms():\n",
    "            for lemma in hypernym.lemmas():\n",
    "                hypernyms_set.add(lemma.name())\n",
    "    \n",
    "    return {\n",
    "        'synsets': list(synset_set),\n",
    "        'hyponyms': list(hyponyms_set),\n",
    "        'hypernyms': list(hypernyms_set)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'data'\n",
    "similars = get_similar_terms_wordnet(token)\n",
    "\n",
    "print(f\"Similares a '{token}':\")\n",
    "for sinonimo in similars['synsets']:\n",
    "    print(f\"- {sinonimo}\")\n",
    "\n",
    "print(f\"\\nHipónimos de '{token}':\")\n",
    "for hiponimo in similars['hyponyms']:\n",
    "    print(f\"- {hiponimo}\")\n",
    "\n",
    "print(f\"\\nHiperónimos de '{token}':\")\n",
    "for hiperónimo in similars['hypernyms']:\n",
    "    print(f\"- {hiperónimo}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
