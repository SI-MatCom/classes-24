{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Laboratorio #4: Procesamiento del Lenguaje Natural\n",
        "\n",
        "El procesamiento del lenguaje natural, también conocido como NLP (por sus siglas en inglés), es un subcampo de la inteligencia artificial que permite a las computadoras comprender el lenguaje humano. \n",
        "\n",
        "EL NLP está a nuestro alrededor: extracción de información, análisis de sentimientos, traducción automática (por ejemplo, el traductor de Google), detección de spam, autocompletar y bots de chat, por nombrar algunos.\n",
        "\n",
        "El lenguaje humano está lleno de ambigüedades, las cuales requerien de análisis complejos para que una computadora pueda entender el significado real de la frase. Por ejemplo, en la oración:\n",
        "<div style=\"text-align:center;\">\n",
        "    <p>La mujer golpeó a un hombre con un paraguas.</p>\n",
        "</div>\n",
        "puede tener dos significados distintos: la mujer golpeó a un hombre el cual tenía un paraguas o, la mujer golpeó con su paraguas a un hombre. Existen algoritmos y técnicas para desambiguar las frases.\n",
        "\n",
        "Muchos de los SRI actuales ofrecen la posibilidad a los usuarios de autocompletar las freses que este introduce y esto lo hacen a través de modelos de lenguajes y representaciones \"cómodas\" que le permiten predecir el texto del usuario. \n",
        "\n",
        "El objetivo de la clase será **implementar un conjunto de funciones que permitan predecir texto**, de forma similar a la mostrada en la figura siguiente: \n",
        "<div style=\"text-align:center;\">\n",
        "    <img src=\"predecir-texto.png\" width=\"250\">\n",
        "</div>\n",
        "\n",
        "Comencemos importando las bibliotecas necesarias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Permitirá el preprocesamiento con los textos\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Función para crear los n-gramas\n",
        "from nltk import ngrams\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Almacenará las relaciones entre la información representada con los n-gramas\n",
        "from nltk.probability import ConditionalFreqDist\n",
        "\n",
        "# Funciones auxiliares útiles \n",
        "from teacher_help import get_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio #1: Implemente una función para procesar textos.\n",
        "\n",
        "Para la resolución del ejercicio puede considerar:\n",
        "- Mantener solo el texto, de la misma forma en que lo encuentre.\n",
        "- No preservar números, signos de puntuación u otros términos con caracteres esepciales.\n",
        "\n",
        "###### Pista: Ya viste algo similar a la respuesta esperada. Intenta recordar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_text(text):\n",
        "    \"\"\"Normalizes the text to be processed\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to process.\n",
        "        \n",
        "    Return:\n",
        "        [str]: Text to process\n",
        "    \n",
        "    \"\"\"\n",
        "    # raise NotImplementedError('Ejercicio #1')\n",
        "    doc = nlp(text)\n",
        "    return [token.text.lower() for token in doc if token.lemma_.isalpha()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto normalizado: ['the', 'tree', 'is', 'empty', 'tomorrow', 'will', 'be', 'a', 'nice', 'day', 'the', 'tree', 'is', 'blue', 'and', 'has', 'dreams']\n"
          ]
        }
      ],
      "source": [
        "text = '''\n",
        "The tree is empty. \n",
        "Tomorrow will be a nice day. \n",
        "The tree is blue and has dreams\n",
        "'''\n",
        "words = normalize_text(text)\n",
        "\n",
        "print('Texto normalizado:', words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "El próximo paso es establecer una relación entre los términos del texto almacenado para la predicción. Nótese que para esta tarea, se necesita una base de datos previa, puesto que el sistema no puede predecir el próximo término de una frase si no conoce de antemano posibles frases.\n",
        "\n",
        "Luego, a través de un ejemplo simple puede demostrarse la dependencia entre algunas palabras, por ejemplo: es más probable que aparezca la frase \"El gato está durmiendo\" que \"El gato es bailando\". Si esta idea se lleva al campo de las probabilidades puede denotarse como:\n",
        "$$P(\\text{``El gato está durmiendo''})>P(\\text{``El gato está bailando''})$$ \n",
        "donde $P$ denota la probabilidad de que la oración.\n",
        "\n",
        "Esta idea puede aplicarse usando el modelo de lenguaje de N-Gramas, el cual predice la probabilidad de la ocurrencia de una secuencia de $N$ términos. Recuerde que la representación BoW (bolsa de palabras) es un modelo donde $N=1$.\n",
        "\n",
        "Para que el modelo N-Grama funcione correctamente se usará la suposición de Markóv, la cual plantea que la probabilidad de que aparezca un término solo dependerá de sus $N-1$ términos anteriores.\n",
        "\n",
        "### Ejercicio #2: Contruya un modelo N-Grama para la representación de la información y sus relaciones.\n",
        "\n",
        "Considere las siguientes ideas durante la resolución de la función:\n",
        "\n",
        "- Se trabajará con n-gramas de tamaño 3.\n",
        "\n",
        "- La función `ngrams` construirá los n-gramas. Para su ejecución, además de la lista de términos, puede hacer uso de otros parámetros como:\n",
        "  - pad_left (boolean): Indica si el primer n-grama detectado será: `('unknown', 'unknown', 'el')` o `('el', 'gato', 'está')`.\n",
        "  - pad_right=(boolean): Indica si el último n-grama detectado será: `('gato', 'está', 'durmiendo')` o `('durmiendo', 'unknown', 'unknown')`.\n",
        "  - left_pad_symbol (str): Si `pad_left=True` define el caracter a poner por 'unknown'.\n",
        "  - right_pad_symbol (str): Si `pad_right=True` define el caracter a poner por 'unknown'.\n",
        "\n",
        "- La clase `ConditionalFreqDist` permite crear un diccionario con cierta particularidad. Un diccionario predeterminado es la estructura de datos subyacente en esta clase y cuenta la frecuencia de co-ocurrencia y pares clave-valor. Almacena los recuentos de una palabra (w3) después de las 2 palabras anteriores (w1, w2) en el trigrama. Si el recuento de w3 no está presente, se utiliza un valor predeterminado. Por ejemplo:\n",
        "  ```python\n",
        "  tmp = ConditionalFreqDist()\n",
        "  tmp[('a', 'b')]['c'] = 4 \n",
        "  tmp[('a', 'b')]['d'] = 2 \n",
        "  tmp[('c', 'd')]['a'] = 1 \n",
        "  total = float(sum(tmp[('a', 'b')].values()))\n",
        "  tmp[('a', 'b')]['c'] /= total\n",
        "\n",
        "  from pprint import pprint\n",
        "  pprint(list(tmp.items()))\n",
        "  ```\n",
        "\n",
        "- Normalice los valores, de forma que lo mantenido en cada elemento de la relación es una probabilidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def n_gram_model(text):\n",
        "    \"\"\"Build the N-Gram model\n",
        "\n",
        "    Args:\n",
        "        text ([str]): List of pre-processed terms.\n",
        "\n",
        "    Returns:\n",
        "        nltk.probability.ConditionalFreqDist: Relationships between n-grams according to the normalized frequency of occurrence\n",
        "    \"\"\"\n",
        "    # raise NotImplementedError('Ejercicio #2')\n",
        "    \n",
        "    trigrams = list(ngrams(text, 3, pad_left=True, pad_right=True, \n",
        "                           left_pad_symbol='<s>', \n",
        "                           right_pad_symbol='</s>'))\n",
        "    \n",
        "    # # Image\n",
        "    # freq_tri = nltk.FreqDist(trigrams)\n",
        "    # freq_tri.plot(30, cumulative=False)\n",
        "    # print(\"Most common trigrams: \", freq_tri.most_common(5))\n",
        "    \n",
        "    # make conditional frequencies dictionary\n",
        "    cfdist = ConditionalFreqDist()\n",
        "    for w1, w2, w3 in trigrams:\n",
        "        cfdist[(w1, w2)][w3] += 1\n",
        "\n",
        "    # transform frequencies to probabilities\n",
        "    for w1_w2 in cfdist:\n",
        "        total_count = float(sum(cfdist[w1_w2].values()))\n",
        "        for w3 in cfdist[w1_w2]:\n",
        "            cfdist[w1_w2][w3] /= total_count\n",
        "\n",
        "    return cfdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Información del modelo:\n",
            "[(('<s>', '<s>'), FreqDist({'the': 1.0})),\n",
            " (('<s>', 'the'), FreqDist({'tree': 1.0})),\n",
            " (('the', 'tree'), FreqDist({'is': 1.0})),\n",
            " (('tree', 'is'), FreqDist({'empty': 0.5, 'blue': 0.5})),\n",
            " (('is', 'empty'), FreqDist({'tomorrow': 1.0})),\n",
            " (('empty', 'tomorrow'), FreqDist({'will': 1.0})),\n",
            " (('tomorrow', 'will'), FreqDist({'be': 1.0})),\n",
            " (('will', 'be'), FreqDist({'a': 1.0})),\n",
            " (('be', 'a'), FreqDist({'nice': 1.0})),\n",
            " (('a', 'nice'), FreqDist({'day': 1.0})),\n",
            " (('nice', 'day'), FreqDist({'the': 1.0})),\n",
            " (('day', 'the'), FreqDist({'tree': 1.0})),\n",
            " (('is', 'blue'), FreqDist({'and': 1.0})),\n",
            " (('blue', 'and'), FreqDist({'has': 1.0})),\n",
            " (('and', 'has'), FreqDist({'dreams': 1.0})),\n",
            " (('has', 'dreams'), FreqDist({'</s>': 1.0})),\n",
            " (('dreams', '</s>'), FreqDist({'</s>': 1.0}))]\n"
          ]
        }
      ],
      "source": [
        "model = n_gram_model(words)\n",
        "\n",
        "from pprint import pprint\n",
        "print('Información del modelo:')\n",
        "pprint(list(model.items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Una vez hecho el modelo, ya es posible predecir frases.\n",
        "\n",
        "### Ejercicio #3: Implemente una función que dado un modelo n-grama y una frase, devuelva una lista del posible término siguiente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(model, phrase):\n",
        "    \"\"\"Predict the next term in a sentence\n",
        "\n",
        "    Args:\n",
        "        model (ConditionalFreqDist): Model.\n",
        "        phrase (_type_): Phrase.\n",
        "\n",
        "    Returns:\n",
        "        [str]: List of possible terms, ordered from highest to lowest probability of appearance.\n",
        "    \n",
        "    \"\"\"\n",
        "    # raise NotImplementedError('Ejercicio #3')\n",
        "    \n",
        "    phrase = normalize_text(phrase)\n",
        "\n",
        "    w1 = len(phrase) - 2\n",
        "    w2 = len(phrase)\n",
        "    prev_words = phrase[w1:w2]\n",
        "\n",
        "    # display prediction from highest to lowest maximum likelihood\n",
        "    return sorted(dict(model[prev_words[0], prev_words[1]]), \n",
        "    key=lambda x: dict(model[prev_words[0], prev_words[1]])[x], \n",
        "    reverse=True)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicción del modelo trigrama:  ['empty', 'blue']\n"
          ]
        }
      ],
      "source": [
        "phrase = 'tree is'\n",
        "predictions = predict(model, phrase)\n",
        "print(\"Predicción del modelo trigrama: \", predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ¿Terminaste? ¡Yeah!\n",
        "\n",
        "Prueba ahora con un ejemplo real. Los textos son libros famosos. Ejecuta la siguiente celda y define tu frase a completar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalizando los textos ...\n",
            "Creando el modelo ...\n"
          ]
        }
      ],
      "source": [
        "print('Normalizando los textos ...')\n",
        "words = get_text('./books', normalize_text)\n",
        "\n",
        "print('Creando el modelo ...')\n",
        "model = n_gram_model(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trigram model predictions:  ['great', 'cordial', 'striking', 'difficult', 'good', 'sweet', 'clear']\n"
          ]
        }
      ],
      "source": [
        "phrase = 'Alice had no very'\n",
        "predictions = predict(model, phrase)\n",
        "print(\"Trigram model predictions: \", predictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
