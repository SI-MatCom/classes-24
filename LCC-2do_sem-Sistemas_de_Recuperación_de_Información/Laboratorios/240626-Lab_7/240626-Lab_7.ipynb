{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio #7: Procesamiento de sonidos\n",
    "\n",
    "El procesamiento de sonidos desempeña un papel fundamental en los Sistemas de Recuperación de Información (SRI). Por ejemplo:\n",
    "\n",
    "- Indexación y búsqueda eficientes: El procesamiento permite extraer características relevantes de los archivos de audio para indexar y buscar eficientemente dentro de grandes conjuntos de datos. \n",
    "- Mejora de la precisión en la recuperación: Al analizar el contenido sonoro de los archivos, los SRI pueden \"entender\" mejor el contexto y el contenido de los archivos de audio. Esto puede mejorar la precisión de las consultas y devolver resultados más relevantes a los usuarios.\n",
    "- Diversificación de los tipos de datos: Con el procesamiento de sonido, los SRI pueden manejar una variedad más amplia de tipos de datos. Esto significa que pueden indexar y recuperar no solo texto, imágenes o videos, sino también archivos de audio, lo que amplía las posibilidades de búsqueda y la experiencia del usuario.\n",
    "- Aplicaciones prácticas: Los SRI basados en el procesamiento de sonido tienen numerosas aplicaciones prácticas, que van desde la búsqueda de música y _podcasts_ hasta la transcripción automática de archivos de audio y la recuperación de información en entornos complejos de multimedia.\n",
    "\n",
    "En resumen, el procesamiento de sonidos en los sistemas de recuperación de información es esencial para entender y organizar el contenido sonoro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facilita el trabajo con los ficheros de sonido\n",
    "import librosa\n",
    "\n",
    "# Funciones necesarias durante la clase\n",
    "from teacher_help import load_data\n",
    "from teacher_help import LSH\n",
    "\n",
    "# Permite graficar funciones\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 1:** Implemente una función para obtener 3 diferentes extractores de características de los sonidos.\n",
    "\n",
    "Cuando se pretende extraer un conjunto de características, es importante extraer las propiedades que son importante para el problema que se trata de resolver. Como el objetivo es realizar la recuperación de sonidos, es necesario escoger características a las que se le pueda aplicar funciones de similitud. Por lo que se recomienda trabajar con los siguientes extractores:\n",
    "\n",
    "- Coeficientes ceptrales de frecuencia Mel\n",
    "- Espectograma de Mel (similar al espectrograma tradicional pero las frecuencias se mapean a la escala de Mel)\n",
    "- Chromagram\n",
    "\n",
    "Estos propiedades son generalmente características de nivel medio, o sea, características con un significado semántico pero que aún conservan información estadística de la señal de audio.\n",
    "\n",
    "###### Pista #1: En todos los análisis quizás le sea necesario trabajar con la señal y con la cantidad de muestras tomadas en el re-muestreo.\n",
    "\n",
    "###### Pista #2: Considere usar el módulo `librosa.feature`. Reutilice algunas representaciones para obtener las otras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio, sr):\n",
    "    \"\"\"\n",
    "    Extracts Mel-frequency cepstral coefficients (MFCCs), log-scaled Mel spectrogram, \n",
    "    and chroma features from an audio signal.\n",
    "\n",
    "    Args:\n",
    "        audio (np.ndarray): \n",
    "            The audio signal as a NumPy array.\n",
    "        sr (int): \n",
    "            Quantity of samples taken.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray, np.ndarray]: \n",
    "            A tuple containing the extracted features:\n",
    "            - mfccs (np.ndarray): The Mel-frequency cepstral coefficients (shape: [n_mfcc, T]).\n",
    "            - log_s (np.ndarray): The log-scaled Mel spectrogram (shape: [n_mels, T]).\n",
    "            - chromagram (np.ndarray): The chroma features (shape: [n_chroma, T]).\n",
    "\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'audio/125_bounce.wav'\n",
    "audio,sr = librosa.load(file)\n",
    "mfccs,mel_spectrum,chromagram = extract_features(audio,sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los MFCC resultantes forman una representación compacta de la señal de audio, capturando características espectrales importantes al tiempo que reducen la dimensionalidad del espacio de características.\n",
    "\n",
    "La gráfica de los coeficientes de Mel representa la distribución de la energía de una señal de audio en diferentes bandas de frecuencia, pero teniendo en cuenta la percepción auditiva humana.\n",
    "\n",
    "En otras palabras, simula cómo el oído humano percibe la intensidad del sonido en diferentes frecuencias.\n",
    "\n",
    "El eje horizontal de la gráfica está en escala de Mel, que es una escala de frecuencias no lineal que comprime las frecuencias bajas y expande las frecuencias altas. Esto se debe a que el oído humano es más sensible a los cambios de tono en las frecuencias bajas que en las altas.\n",
    "\n",
    "El eje vertical de la gráfica representa la magnitud de la energía en cada banda de frecuencia, expresada en decibelios (dB). Los valores más altos indican que hay más energía en esa banda de frecuencia.\n",
    "\n",
    "Al observar la gráfica de los coeficientes de Mel, se puede obtener información sobre el contenido espectral de la señal de audio. Por ejemplo:\n",
    "- Picos en la gráfica: Indican que hay una gran cantidad de energía en esas bandas de frecuencia específicas.\n",
    "- Forma general de la gráfica: Puede revelar características como la sonoridad (cantidad total de energía) o la tonalidad (distribución de la energía a lo largo de las frecuencias).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los MFCCs\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mfccs, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCCs')\n",
    "plt.xlabel('Tiempo')\n",
    "plt.ylabel('Coeficientes de Mel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La escala Mel (el nombre Mel proviene de la palabra melodía) es una escala perceptiva de tonos que el oído humano considera iguales en distancia entre sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.specshow(mel_spectrum, sr=sr, x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Espectrograma de Mel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Un diagrama de cromagrama representa la distribución de la energía tonal de una señal de audio en diferentes tonos musicales a lo largo del tiempo. Se obtiene mediante la transformación de Fourier de corto plazo (STFT) de la señal de audio y luego calculando la magnitud de la energía en cada tono musical. Los tonos musicales se basan en la escala cromática occidental estándar, que consta de 12 notas (do, re, mi, fa, sol, la, si, do, re, mi, fa, sol, la, si) en un ciclo repetitivo.\n",
    "\n",
    "El eje vertical del diagrama de cromagrama está dividido en 12 secciones, cada una representando un tono musical de la escala cromática.\n",
    "\n",
    "El eje horizontal del diagrama de cromagrama representa el tiempo. Cada columna vertical del diagrama corresponde a un segmento de tiempo corto de la señal de audio. Los colores o la intensidad de los valores en cada sección de tono musical a lo largo del tiempo indican la cantidad de energía tonal presente en ese tono en ese segmento de tiempo específico.\n",
    "\n",
    "Al observar un diagrama de cromagrama, se puede obtener información sobre la tonalidad y la armonía de la señal de audio. Por ejemplo:\n",
    "- Colores intensos en un tono musical específico: Indican que hay una gran cantidad de energía en ese tono musical en ese segmento de tiempo.\n",
    "- Patrones a lo largo del tiempo: Pueden revelar la progresión de acordes, melodías o cambios tonales en la música.\n",
    "- Comparación entre diferentes filas: Permite comparar la distribución de energía tonal en diferentes segmentos de tiempo de la señal de audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.specshow(chromagram,x_axis='time',y_axis='chroma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Locality Sensitive Hashing (LSH) es uno de los métodos de búsqueda aproximada de vecinos más cercanos (ANNS) más populares.\n",
    "\n",
    "En esencia, es una función hash que permite agrupar elementos similares en los mismos depósitos hash con una alta probabilidad. Luego, dado un conjunto de datos increíblemente grande, se procesan todos los elementos a través de la función hash, agrupándolos en depósitos.\n",
    "\n",
    "A diferencia de la mayoría de las funciones hash, que tienen como objetivo minimizar las colisiones hash, los algoritmos LSH tienen como objetivo maximizar las colisiones hash.\n",
    "\n",
    "El resultado esperado para LSH es que los vectores similares generen el mismo valor de hash y por lo tanto son agrupados en el mismo depósito. Por el contrario, vectores diferentes \"probablemente\" no generaran el mismo valor de hash, siendo colocados en depósitos distintos.\n",
    "\n",
    "#### Búsqueda con LSH\n",
    "Realizar la búsqueda con LSH consiste de 3 pasos:\n",
    "1. Indexar todos los vectores en su representación hash.\n",
    "2. Representar el vector query, o el término de búsqueda, luego de ser procesado por la función hash (la misma utilizada en LSH).\n",
    "3. Comparar la nueva representación del vector query con todos los otros depósitos, identificando el más cercano.\n",
    "\n",
    "### **Ejercicio 2:** Implementes las funciones faltantes de la clase MusicSearch.\n",
    "\n",
    "Para ello, debe de auxiliarse de las clases definidas en el fichero _teacher_help_. \n",
    "\n",
    "###### Nota: Se debe tener en cuenta a la hora de implementar ambos métodos en que función de extracción de features seleccionar. En el inicializador de clase se encuentran los parámetros hop_size y frame_size(n_fft) que serán útiles para algunos de los métodos ya vistos en clase. Aunque su uso no es obligatorio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicSearch:\n",
    "    \"\"\"\n",
    "    Implements a music search system using Locality-Sensitive Hashing (LSH).\n",
    "\n",
    "    Attributes:\n",
    "        frame_size (int): \n",
    "            The size of the audio window used for feature extraction.\n",
    "        hop_size (int): \n",
    "            The hop size between consecutive windows during feature extraction.\n",
    "        fv_size (int): \n",
    "            The feature vector size (number of chroma features).\n",
    "        lsh (LSH): \n",
    "            An instance of the LSH class used for approximate nearest neighbor search.\n",
    "        training_files (list): \n",
    "            A list of file paths for the training dataset.\n",
    "        num_features_in_file (dict): \n",
    "            A dictionary storing the number of features extracted from each training file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, training_files: list):\n",
    "        \"\"\"\n",
    "        Initializes the MusicSearch object with the training data file paths.\n",
    "\n",
    "        Args:\n",
    "            training_files (list): A list of file paths for the training music dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        self.frame_size = 4096\n",
    "        self.hop_size = 4000\n",
    "        self.fv_size = 12\n",
    "        self.lsh = LSH(self.fv_size)\n",
    "        self.training_files = training_files\n",
    "        self.num_features_in_file = dict()\n",
    "        for f in self.training_files:\n",
    "            self.num_features_in_file[f] = 0\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the music search system by extracting chroma features and adding them to the LSH index.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: (Placeholder, can be removed if implemented) This placeholder\n",
    "                can be removed if the function is actually implemented.\n",
    "        \"\"\"\n",
    "        # This function iterates through all training files, extracts X features using librosa, and adds them to the LSH index along with the corresponding file path as a label. It also keeps track of the number of features extracted from each file.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def query(self, filepath: str) -> dict:\n",
    "        \"\"\"\n",
    "        Performs a music search query on a given audio file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): The file path of the query audio file.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the query results. Keys are file paths of similar music,\n",
    "                   and values are scores representing their similarity to the query.\n",
    "        \"\"\"\n",
    "        #  This function extracts chroma features from the query file and uses the LSH index to search for similar features in the training data. It retrieves potential matches and calculates a score for each based on the frequency of their occurrence in the results.\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = load_data()\n",
    "ms = MusicSearch(training_files)\n",
    "ms.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Ejercicio 3:** Devolver un ranking de los resultados según la cantidad de veces que cada archivo aparece representado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'audio/drum_samples/test/kick_01.mp3'\n",
    "\n",
    "NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
