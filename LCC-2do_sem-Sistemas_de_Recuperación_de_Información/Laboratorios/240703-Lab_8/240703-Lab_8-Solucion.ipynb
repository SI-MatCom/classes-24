{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Idea tomada de https://www.zenrows.com/blog/web-crawler-python#distributed-web-scraping-in-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalando las dependencias ...\n",
        "\n",
        "Durante la clase se estar치n utilizando las siguientes bibliotecas:\n",
        "- `requests` permite enviar solicitudes HTTP f치cilmente.\n",
        "- `beautifulsoup4` facilita la extracci칩n de informaci칩n sint치ctica de p치ginas web analizando el HTML.\n",
        "  \n",
        "Compruebe que tenga las bibliotecas o inst치lelas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4 requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Laboratorio #8: Web Crawling y Web Scraping\n",
        "\n",
        "Web crawling y Web scraping son dos t칠rminos relacionados pero distintos en el 치mbito de la extracci칩n de datos en la Web. \n",
        "\n",
        "**Web Crawling** (Rastreo web):\n",
        "- Proceso automatizado de explorar la Web y recopilar informaci칩n de diversas p치ginas web.\n",
        "- Los programas son conocidos como \"ara침as\" o \"rastreadores web\" y navegan por la web siguiendo enlaces de una p치gina a otra.\n",
        "- Tiene como objetivo principal indexar el contenido de la web para los motores de b칰squeda, de forma que los motores de b칰squeda puedan recopilar datos sobre p치ginas web y actualizar sus 칤ndices.\n",
        "- Es un proceso m치s amplio que el scraping porque se enfoca en recorrer y descubrir nuevas p치ginas, en lugar de extraer datos espec칤ficos de las p치ginas.\n",
        "  \n",
        "**Web Scraping** (Extracci칩n web):\n",
        "- Proceso de extracci칩n de datos espec칤ficos de p치ginas web.\n",
        "- Se utiliza para recopilar informaci칩n estructurada de una p치gina web como: texto, im치genes, enlaces, precios de productos, documentos, enlaces, etc.\n",
        "- A diferencia del rastreo web, este se centra en extraer informaci칩n de p치ginas web espec칤ficas.\n",
        "- Se utiliza en una variedad de casos de uso como la recopilaci칩n de datos para el an치lisis, la monitorizaci칩n de precios en l칤nea, la investigaci칩n de mercado, entre otros.\n",
        "\n",
        "Esta clase se enfocar치 en esos dos temas. Para ello tiene la siguiente problem치tica:\n",
        "> Los profesores de la asignatura de SRI han decidido fundar una Mipyme para vender peluches de Pokemon porque se han percatado que MatCom es un nicho sin explotar. Como es un negocio emergente, se ha decido comprar los peluches en Inglaterra. Los precios de los peluches de Pokemon fluct칰an en el mercado y se requiere recopilar datos para confeccionar estad칤sticas para asegurar y maximizar las ganancias de los profesores. \n",
        "\n",
        "\n",
        "### Ejercicio #1:\n",
        "Construya una tabla con los datos de cada Pokemon. Puede tomar la informaci칩n de `https://scrapeme.live/shop/`. De cada peluche se quiere recuperar: \n",
        "- nombre del Pokemon, \n",
        "- precio en libras esterlinas, \n",
        "- descripci칩n del producto, \n",
        "- url donde est치 alojada una foto del Pokemon, y \n",
        "- url de donde se tomaron los datos.\n",
        "\n",
        "Recuerde que la empresa es nueva por lo que tiene pocos recursos. Por tanto, es requerimiento:\n",
        "- no visitar una p치gina visitada anteriormente.\n",
        "- no consultar p치ginas web fuera de la tienda.\n",
        "- activar el proceso solo una vez, de forma manual.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os.path import join\n",
        "import csv\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pista #1:\n",
        "Culmine de implementar la clase `MyQueue`, la cual definir치 una pol칤tica de ordenaci칩n de las URLs.\n",
        "\n",
        "*쯈u칠 es la pol칤tica de ordenaci칩n de URL?*\n",
        "\n",
        "*쯈u칠 le aporta al crawler?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyQueue:\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize a new instance\n",
        "        \n",
        "        \"\"\"\n",
        "        # raise Exception('Not Implemented')\n",
        "        self._queue = list()\n",
        "        self._memory = list()\n",
        "        \n",
        "    def pop(self):\n",
        "        \"\"\"\n",
        "        Gets and returns an element\n",
        "\n",
        "        Return:\n",
        "            - t\n",
        "            \n",
        "        \"\"\"\n",
        "        # raise Exception('Not Implemented')\n",
        "        if self._queue:\n",
        "            elem = self._queue.pop(0)\n",
        "            self._memory.append(elem)\n",
        "            return elem\n",
        "        else:\n",
        "            return None\n",
        "        \n",
        "    def add(self, elem):\n",
        "        \"\"\"\n",
        "        Adds an element to the queue\n",
        "\n",
        "        Arg:\n",
        "            - elem (t) : Element to add.\n",
        "            \n",
        "        \"\"\"\n",
        "        # raise Exception('Not Implemented')\n",
        "        if elem in self._queue or elem in self._memory:\n",
        "            pass\n",
        "        else:\n",
        "            self._queue.append(elem)\n",
        "            \n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of items stored\n",
        "\n",
        "        Return:\n",
        "            - int\n",
        "            \n",
        "        \"\"\"\n",
        "        return len(self._queue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Funci칩n de regalo para almacenar datos en un fichero CSV. Luego de culminarse la clase, los profesores aprovechar치n los datos recopilados para comenzar la inversi칩n. \n",
        "# 游땓游눯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def write_to_csv(data, path='.', file_name='pokemon-products'):\n",
        "    \"\"\"\n",
        "    Write data to a CSV file\n",
        "\n",
        "    Arg:\n",
        "        - data (list<dict>) : List of dictionaries. It is assumed that each dictionary has the same keys.\n",
        "\n",
        "    \"\"\"\n",
        "    with open(join(path, file_name + '.csv'), 'w', newline='', encoding='utf-8') as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "\n",
        "        # Escribir encabezados (suponiendo que todos los productos tienen las mismas claves)\n",
        "        if data:\n",
        "            writer.writerow(data[0].keys())\n",
        "\n",
        "        # Escribir los valores de los productos en el archivo CSV\n",
        "        for d in data:\n",
        "            writer.writerow(d.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lista para almacenar los datos extra칤dos de los productos de Pokemon\n",
        "products = []\n",
        "\n",
        "# URL base de la p치gina web a scrapear\n",
        "base_url = 'https://scrapeme.live/shop/'\n",
        "\n",
        "# Inicializaci칩n de una cola para almacenar las URLs a visitar\n",
        "urls = MyQueue()\n",
        "urls.add('https://scrapeme.live/shop/') # Agregar la primera URL a la cola: conjunto semilla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La celda siguiente es para que implemente el proceso de recaudaci칩n de datos.\n",
        "\n",
        "### Pista #2.x\n",
        "1. Utilice la clase `MyQueue` para almacenar los hiperv칤nculos extra칤dos de cada sitio web y extraer la p치gina web a visitar.\n",
        "   \n",
        "2. La funci칩n `requests.get/1` dada una url, obtiene todo el c칩digo HTML de la p치gina. Para obtener ese c칩digo basta consultarle la propiedad `content` a lo que retorna.\n",
        "   \n",
        "3. Utilizando `BeautifulSoup(contenido_html, 'html.parser')` obtienes un objeto c칩modo para consultar el c칩digo HTML utilizando etiquetas y clases espec칤ficas.\n",
        "   \n",
        "4. Para obtener los enlaces de la p치gina web puede hacer `soup.select('a[href]')`. De cada elemento que devuelva ese m칠todo, si desea manejar directamente el hiperv칤nculo, indexe el objeto en `'href'`.\n",
        "   \n",
        "5. Para obtener el nombre del Pokemon puede hacer `soup.find('h1', class_='product_title entry-title').text`. \n",
        "   \n",
        "6. Para obtener el precio del peluche puede hacer `soup.find('p', class_='price').text.strip()`.\n",
        "   \n",
        "7. Para obtener la descripci칩n del juguete puede hacer `soup.find('div', class_='woocommerce-product-details__short-description').text.strip()`.\n",
        "    \n",
        "8.  Para obtener la imagen del Pokemon puede hacer `soup.select_one('.wp-post-image')['src']`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pista #3:\n",
        "Se recomienda:\n",
        "- Discutir el flujo del proceso antes de implementarlo.\n",
        "- Detener el proceso cuando `products` tenga 10 elementos porque la Mipyme aun no ha dado recompensas y los directivos no pagar치n el uso de los datos m칩viles particulares."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# raise Exception('Not Implemented')\n",
        "\n",
        "while len(urls) != 0:\n",
        "    \n",
        "    # Obtener la URL inmediata seg칰n el criterio de ordenaci칩n\n",
        "    current_url = urls.pop()\n",
        "\n",
        "    # Realizar una solicitud GET a la URL actual y parsear el contenido HTML\n",
        "    response = requests.get(current_url, verify=False)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    \n",
        "    # Verificar si la p치gina contiene reCAPTCHA\n",
        "    if \"recaptcha\" in soup.text.lower():\n",
        "        # Si se detecta reCAPTCHA, imprimir un mensaje y esperar un tiempo aleatorio antes de continuar\n",
        "        print(\"춰Se detect칩 reCAPTCHA en la p치gina! Esperando para continuar...\")\n",
        "        time.sleep(random.randint(5, 10)) # Esperar entre 5 y 10 segundos. La soluci칩n real es utilizar un servicio de terceros para resolverlo\n",
        "\n",
        "    # Obtener todos los hiperv칤nculos que la p치gina posee\n",
        "    link_elements = soup.select('a[href]')\n",
        "\n",
        "    # Iterar sobre los elementos de enlace encontrados\n",
        "    for link_element in link_elements:\n",
        "        url = link_element['href']\n",
        "        \n",
        "        # Si la URL pertenece al mismo dominio, agregarla a la cola para ser visitada\n",
        "        if base_url in url:\n",
        "            urls.add(url)\n",
        "    \n",
        "    try:\n",
        "        # Extraer informaci칩n del producto de la p치gina\n",
        "        product = {}\n",
        "        product['name'] = soup.find('h1', class_='product_title entry-title').text\n",
        "        product['price'] = soup.find('p', class_='price').text.strip()\n",
        "        product['image'] = soup.select_one('.wp-post-image')['src']\n",
        "        product['url'] = current_url\n",
        "        product['description'] = soup.find('div', class_='woocommerce-product-details__short-description').text.strip()\n",
        "\n",
        "        # Agregar el producto a la lista de productos\n",
        "        products.append(product)\n",
        "        \n",
        "        # # Para limitar el gasto de los datos de los estudiantes\n",
        "        # if len(products) == 10:\n",
        "        #     break\n",
        "        \n",
        "    except Exception as error:\n",
        "        # Capturar y manejar cualquier error que ocurra durante el proceso de extracci칩n\n",
        "        # Se espera un error cuando la p치gina no es de producto, sino que sirve de cat치logo \n",
        "        print(\"Se ha producido un error. \\nUrl: \", current_url, '\\nError: ', error, '\\n')\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Escribir los productos extra칤dos en un archivo CSV\n",
        "write_to_csv(products)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio #2:\n",
        "a) 쮺칩mo modificar el algoritmo para implementar la pol칤tica de amabilidad? 쯈u칠 regla(s) implementar칤as?\n",
        "\n",
        "b) 쯊iene sentido implementar alguna pol칤tica de revisitado? De ser positiva la respuesta, 쯖u치l ser칤a?\n",
        "\n",
        "c) 쯃a soluci칩n propuesta le otorga prioridad a las URLs para visitarlas?\n",
        "\n",
        "d) 쯃a soluci칩n propuesta contempla una demora entre cada pedido o consulta? De ser negativa la respuesta y teniendo en cuenta que eso implica que una carga excesiva para el servidos y puede bloquear el IP del proceso en el cual se ejecuta el crawler, 쯤u칠 puede hacer para controlar esto?\n",
        "\n",
        "e) 쮼xiste alg칰n mecanismo para extraer la informaci칩n en un menor tiempo? 쯈u칠 problemas pueden crearse?\n",
        "\n",
        "f) 쮼l crawler es persistente y robusto?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Existe herramientas que permiten hacer estos procesos. Estas son:\n",
        "- ZenRows: \n",
        "  - API integral de rastreo y raspado. \n",
        "  - Ofrece proxies rotativos, geolocalizaci칩n, renderizado de JavaScript y bypass antibloqueo avanzado.\n",
        "- Scrapy: \n",
        "  - Opci칩n de biblioteca de rastreo en Python, muy poderosa para principiantes. \n",
        "  - Proporciona un marco de alto nivel para crear rastreadores escalables y eficientes.\n",
        "- Selenium: \n",
        "  - Biblioteca popular de navegador para rastreo y raspado web. \n",
        "  - A diferencia de BeautifulSoup, puede interactuar con p치ginas web en un navegador como lo har칤an los usuarios humanos.\n",
        "  - Est치 implementada para dis칤miles lenguajes de programaci칩n."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
